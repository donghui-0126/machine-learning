{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40903730",
   "metadata": {},
   "source": [
    "휴학을 하고 여러 공부를 했다.\n",
    "\n",
    "나의 휴학 목표는 소프트웨어융합학과 복수전공을 내년 상반기에 시작하는 것 + 코딩공부가 향상이기 때문에 내가 관심있는 머신러닝 분야를 많이 공부했다. \n",
    "\n",
    "지금까지 한 공부로는\n",
    "1. 코드잇 머신러닝을 통한 머신러닝 공부\n",
    "2. 위에 공부에서 부족함을 느껴서 [파이썬 머신러닝 완벽가이드](https://github.com/donghui-0126/machine-learning/tree/main/%ED%8C%8C%EC%9D%B4%EC%8D%AC%20%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EC%99%84%EB%B2%BD%20%EA%B0%80%EC%9D%B4%EB%93%9C)책을 이용한 머신러닝 공부\n",
    "3. \\[[카페 사진 감성 분석](https://github.com/donghui-0126/mini-project/tree/main/%EC%B9%B4%ED%8E%98%20%EC%82%AC%EC%A7%84%20%EA%B0%90%EC%84%B1%20%EB%B6%84%EC%84%9D)\\]프로젝트\n",
    "\n",
    "를 했다. \n",
    "\n",
    "\n",
    "이 공부를 하고 다음 공부를 무엇을 할 지 후보를 정해봤다. \n",
    "- 머신러닝을 좀더 자세하게 공부하기\n",
    "- 딥러닝에 대해서 공부하기\n",
    "- html/css/javascript(웹) 공부하기\n",
    "- django 라이브러리 공부하기\n",
    "가 후보 군에 있었는데, 나는 딥러닝을 공부해보고 싶었다.\n",
    "\n",
    "그 이유는 전에 dacon에서 진행되는 머신러닝 대회([청경채 성장 분석](https://dacon.io/competitions/official/235961/codeshare/6021?page=1&dtype=recent))를 하려고 했다.<br>\n",
    "그때 나는 내가 배운 머신러닝으로 작성할 엄두안났는데(물론 나의 머신러닝실력도 하찮지만..) 예제 코드를 보니 딥러닝 기법을 사용한 코드가 적혀있었다. \n",
    "\n",
    "그것을 보고 충격을 받아서 딥러닝도 공부하기로 결심했다. 꼭 그거 때문만은 아니고 재밌어 보여서도 있다! ㅎㅎ ~~공부n일차: 상당히 재미지다.~~\n",
    "\n",
    "딥러닝은 10장 11장 12장을 공부하고 다시 머신러닝을 공부할 생각이다. \n",
    "\n",
    "아마 머신러닝의 추가적인 공부/프로젝트/웹/django(백엔드) 공부를 병행할 것 같다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e55aea2",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4533e71b",
   "metadata": {},
   "source": [
    "이제 본격적인 공부를 시작할 건데, 나는 핸즈온 머신러닝 으로 공부하기로 했다.\n",
    "\n",
    "이 책이 좋다고 하던데 마침 전공책으로도 쓰여서 미리 봐두면 좋을 것 같기 때문이다. \n",
    "\n",
    "책에다 밑줄을 그으면서 공부하기 때문에, 여기는 굉장히 중요하다는 개념이나 코드들만 적힐 것이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d881af",
   "metadata": {},
   "source": [
    "-----------\n",
    "이전에 간단하게 공부했던 deep-leraning 정리본 링크이다.<br>\n",
    "https://github.com/donghui-0126/machine-learning/tree/main/deep-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71964f26",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6fa119",
   "metadata": {},
   "source": [
    "### 연습문제 1 \n",
    "https://playground.tensorflow.org"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ed581f",
   "metadata": {},
   "source": [
    "### 텐서플로우 설치하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48128f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n",
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a004269",
   "metadata": {},
   "source": [
    "### 케라스를 사용해서 데이터셋 적재하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f26ddbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist= keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f98e741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape\n",
    "# 784크기(1d)로 데이터가 나열된 사이킷런의 mnist데이터(matrix)와 다르게 28x28 배열로 이루어짐(tensor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14bfd067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f04acdb",
   "metadata": {},
   "source": [
    "### 트레인셋을 검정셋과 트레인셋으로 나누고 0~1사이의 범위로 스케일 변환\n",
    "검증셋이 없으면 트레인셋에 대한 과대적합이 일어날 수도 있기 때문에 유의해야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2057cdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:]/255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab827841",
   "metadata": {},
   "source": [
    "### class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b7e473a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \" Shirt\",\"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c860cb2b",
   "metadata": {},
   "source": [
    "### 시퀀셜 API를 사용해서 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8a3fa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential 모델을 만든다. 이 모델을 가장 간단한 케라스의 신경망 모델이다.\n",
    "#순서대로 연결된 층을 일렬로 쌓아서 구성한다.\n",
    "model = keras.models.Sequential() \n",
    "\n",
    "# 첫번째 층을 만들고 모델에 추가함. Flatten층은 입력이미지를 1D 배열로 변환한다. \n",
    "# 즉, 입력 데이터 X를 받으면 X.reshape(-1, 28*28)을 계산한다. 이층은 모델 파라미터를 가지지 않고, 간단한 전처리만 수행한다.\n",
    "# 모델의 첫번째 층이므로 input_shape를 지정해야한다. 여기에는 배치 크기를 제외한 샘플의 크기만 써야한다. \n",
    "model.add(keras.layers.Flatten(input_shape=[28,28]))\n",
    "\n",
    "# 뉴런 300개를 가진 Dense 은닉층을 추가한다. 이 층의 활성화 함수는 ReLU가 사용된다. \n",
    "# Dense층 마다 각자 가중치행렬을 관리한다. 이 행렬에는 뉴런과 입력 사이의 모든 연결 가중치가 포함된다. \n",
    "# 또한 뉴런마다 하나씩 있는 편향도 벡터로 관리한다.\n",
    "model.add(keras.layers.Dense(300,activation='relu'))\n",
    "\n",
    "# 다음층도 뉴런 100개를 가진 활성화 함수가 ReLU인 Dense은닉층을 추가한다.\n",
    "model.add(keras.layers.Dense(100,activation='relu'))\n",
    "\n",
    "# 마지막으로 뉴런 10개를 가진 Dense층을 추가한다. 배타적인 클래스이기 때문에 soft_max함수를 사용한다.\n",
    "model.add(keras.layers.Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39fbdb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위와 같은 코드를 다른 방식으로 작성하는 코드이다.\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dense(300, activation='relu'),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7931bee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "# dense_6은 784 * 300 (가중치 개수) + 300 (편향개수) =235500 만큼의 파라미터를 가진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "858145a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.reshaping.flatten.Flatten at 0x20b92aebe50>,\n",
       " <keras.layers.core.dense.Dense at 0x20b92aebfd0>,\n",
       " <keras.layers.core.dense.Dense at 0x20b92aebd60>,\n",
       " <keras.layers.core.dense.Dense at 0x20b93099b50>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963283a1",
   "metadata": {},
   "source": [
    "### 각 층에 인덱스로 접근"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faeda482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense_3'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1 = model.layers[1]\n",
    "hidden1.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c57c9e3",
   "metadata": {},
   "source": [
    "### 각 층의 가중치과 편향을 얻는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcbb2c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.00421565, -0.00351667,  0.05101556, ..., -0.0108448 ,\n",
       "         0.01602399, -0.03167915],\n",
       "       [ 0.00324626,  0.01552586,  0.04142319, ..., -0.07091267,\n",
       "        -0.02079091, -0.00040959],\n",
       "       [ 0.02905273, -0.05547583,  0.03890333, ..., -0.04754068,\n",
       "         0.01252925,  0.03921711],\n",
       "       [-0.05560946, -0.05324187, -0.03196876, ...,  0.07071427,\n",
       "         0.04005713,  0.0312743 ],\n",
       "       [ 0.01593808, -0.02285301, -0.04381575, ...,  0.04904685,\n",
       "        -0.03033808,  0.03853341]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W, b = hidden1.get_weights()\n",
    "print(W.shape)\n",
    "W[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be5d26f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(b.shape)\n",
    "b[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec681b84",
   "metadata": {},
   "source": [
    "### 모델 컴파일\n",
    "사용할 손실함수와 옵티마이저를 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6da2e8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "             optimizer=\"sgd\",\n",
    "             metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f29a39",
   "metadata": {},
   "source": [
    "위 코드에 대한 설명이 필요할 것 같다.\n",
    "\n",
    "위 코드에서 클래스가 배타적이므로 sparse_categorical_crossentropy 를 사용한다. <br>\n",
    "만약 샘플마다 클래스 별 타깃 확률을 가지고 있다면 catergorical_crossentropy 손실을 사영해야한다.<br>\n",
    "이진 분류나 다중 레이블 이진 분류를 수행한다면 출력층에 \"softmax\"대신 \"sigmoid\"를 사용하고 \"binary_crossentropy\"손실을 사용한다.\n",
    "\n",
    "옵티마이저에 \"sgd\"를 지정하면 기본 확률적 경사하강법을 사용해서 모델을 훈련한다는 의미이다. 즉, 역전파 알고리즘을 수행하는 것이다.\n",
    "\n",
    "마지막으로 분류기 이므로 훈련과 평가시에 정확도를 측정하기 위해서 \"accuracy\"로 지정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517337fe",
   "metadata": {},
   "source": [
    "### 모델훈련과 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcd2852e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.7261 - accuracy: 0.7597 - val_loss: 0.5212 - val_accuracy: 0.8224\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.4909 - accuracy: 0.8296 - val_loss: 0.5058 - val_accuracy: 0.8264\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.4438 - accuracy: 0.8458 - val_loss: 0.4042 - val_accuracy: 0.8634\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.4146 - accuracy: 0.8540 - val_loss: 0.4318 - val_accuracy: 0.8472\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.3942 - accuracy: 0.8626 - val_loss: 0.3879 - val_accuracy: 0.8638\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.3781 - accuracy: 0.8663 - val_loss: 0.3760 - val_accuracy: 0.8674\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.3630 - accuracy: 0.8706 - val_loss: 0.3616 - val_accuracy: 0.8704\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.3518 - accuracy: 0.8752 - val_loss: 0.3578 - val_accuracy: 0.8752\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.3416 - accuracy: 0.8792 - val_loss: 0.3407 - val_accuracy: 0.8792\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.3312 - accuracy: 0.8819 - val_loss: 0.3378 - val_accuracy: 0.8802\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.3232 - accuracy: 0.8848 - val_loss: 0.3629 - val_accuracy: 0.8724\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.3155 - accuracy: 0.8870 - val_loss: 0.3497 - val_accuracy: 0.8736\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.3072 - accuracy: 0.8891 - val_loss: 0.3305 - val_accuracy: 0.8782\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.2999 - accuracy: 0.8931 - val_loss: 0.3315 - val_accuracy: 0.8832\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.2943 - accuracy: 0.8938 - val_loss: 0.3225 - val_accuracy: 0.8876\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.2894 - accuracy: 0.8958 - val_loss: 0.3142 - val_accuracy: 0.8892\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.2829 - accuracy: 0.8978 - val_loss: 0.3385 - val_accuracy: 0.8780\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.2773 - accuracy: 0.8985 - val_loss: 0.3052 - val_accuracy: 0.8940\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.2723 - accuracy: 0.9034 - val_loss: 0.3020 - val_accuracy: 0.8930\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.2666 - accuracy: 0.9053 - val_loss: 0.3062 - val_accuracy: 0.8886\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.2619 - accuracy: 0.9053 - val_loss: 0.3374 - val_accuracy: 0.8778\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.2577 - accuracy: 0.9077 - val_loss: 0.3004 - val_accuracy: 0.8918\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.2532 - accuracy: 0.9086 - val_loss: 0.3051 - val_accuracy: 0.8906\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.2479 - accuracy: 0.9103 - val_loss: 0.3014 - val_accuracy: 0.8896\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2439 - accuracy: 0.9122 - val_loss: 0.3044 - val_accuracy: 0.8906\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2407 - accuracy: 0.9134 - val_loss: 0.2978 - val_accuracy: 0.8916\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.2355 - accuracy: 0.9151 - val_loss: 0.2959 - val_accuracy: 0.8960\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2320 - accuracy: 0.9170 - val_loss: 0.3146 - val_accuracy: 0.8846\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.2278 - accuracy: 0.9172 - val_loss: 0.3024 - val_accuracy: 0.8876\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.2244 - accuracy: 0.9191 - val_loss: 0.2996 - val_accuracy: 0.8910\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                   validation_data = (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9ebed1",
   "metadata": {},
   "source": [
    "<b>특정 클래스가 조금 등장하는 비대칭적 데이터의 경우에는 fit()메소드를 호출할 때 class_weight 매개변수를 지정하는 것이 좋다. 샘플별로 가중치를 부여하고 싶다면 sample_weight 매개변수를 지정하면 된다. (두값이 모두 지정되면 곱해서 사용한다.)<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5967d28",
   "metadata": {},
   "source": [
    "### 모델을 다시 fit하면 이어서 fit 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d47c3a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.2203 - accuracy: 0.9203 - val_loss: 0.3215 - val_accuracy: 0.8858\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.2165 - accuracy: 0.9229 - val_loss: 0.2899 - val_accuracy: 0.8974\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.2133 - accuracy: 0.9243 - val_loss: 0.2883 - val_accuracy: 0.8970\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.2103 - accuracy: 0.9250 - val_loss: 0.2928 - val_accuracy: 0.8978\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.2063 - accuracy: 0.9267 - val_loss: 0.2921 - val_accuracy: 0.8964\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.2039 - accuracy: 0.9271 - val_loss: 0.2951 - val_accuracy: 0.8962\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.2006 - accuracy: 0.9282 - val_loss: 0.2938 - val_accuracy: 0.8988\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.1968 - accuracy: 0.9309 - val_loss: 0.2979 - val_accuracy: 0.8964\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.1935 - accuracy: 0.9311 - val_loss: 0.3152 - val_accuracy: 0.8918\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.1905 - accuracy: 0.9317 - val_loss: 0.2975 - val_accuracy: 0.8920\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20b92d70d30>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10,\n",
    "                   validation_data = (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97f6361",
   "metadata": {},
   "source": [
    "### fit() 메소드가 반환하는 History 객체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "038836e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True) # 격자 생성\n",
    "plt.gca().set_ylim(0,1) # 수직축의 범위를 0~1로 지정\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01a21d9",
   "metadata": {},
   "source": [
    "모델의 성능이 너무 낮다면 일반적으로\n",
    "1. 학습률\n",
    "2. 다른 옵티마이저\n",
    "3. 층개수, 뉴런개수, 은닉층의 활성화함수\n",
    "4. 배치크기 \n",
    "순으로 하이퍼라미터를 수정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ce263c",
   "metadata": {},
   "source": [
    "### evaluate() 메소드\n",
    "테스트세트로 모델평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "422af5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3312 - accuracy: 0.8864\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3311586081981659, 0.8863999843597412]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c20f63d",
   "metadata": {},
   "source": [
    "~~엥... 예측 성능이 너무 낮다..~~\n",
    "\n",
    "~~이거를 검색해 봤는데 그냥 진행바에 오류가 있다고 하는데.. 맞는지 모르겠다.~~\n",
    "~~- https://stackoverflow.com/questions/44384924/model-evaluate-in-keras-do-not-cover-all-datapoints~~\n",
    "\n",
    "\n",
    "~~이거는 뉴런들이 dropout 된다는데.. 잘모르겠다~~\n",
    "~~- https://github.com/keras-team/keras/issues/6977~~\n",
    "\n",
    "다시 실행하니까 된다. 이런;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9113c429",
   "metadata": {},
   "source": [
    "### 모델을 사용해서 예측을 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fcaf763",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 172ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e387406",
   "metadata": {},
   "source": [
    "### predict_classes 메소드\n",
    "2.6.0 버전이후로 이 메소드가 없어졌다. 그냥 predict메소드에 np.argmax를 적용해서 사용하면 된다"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5edfbff0",
   "metadata": {},
   "source": [
    "y_pred = model.predict_classes(y_proba)\n",
    "\n",
    "#이렇게 사용하면 됨!\n",
    "y_pred = np.argmax(y_proba,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ade67536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.argmax(y_proba,axis=-1)\n",
    "y_pred\n",
    "np.array(class_names)[y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21c73ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1], dtype=uint8)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_new = y_test[:3]\n",
    "y_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024255d3",
   "metadata": {},
   "source": [
    "### 시퀀셜 API를 사용하여 회귀용 다층 퍼셉트론(MLP) 만들기\n",
    "캘리포니아 주택가격"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e3daf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_vaild = scaler.fit_transform(X_valid)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
    "X_new = X_test[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9d94e2",
   "metadata": {},
   "source": [
    "<b>분류와 회귀의 차이점은 출력층이 활성화 함수가 없는 하나의 뉴런을 가진다는 것과 손실함수로 평균 제곱 오차를 사용한다는 것이다. <br> 이 데이터는 오차가 많기 때문에 뉴런이 적은 하나의 은닉층을 사용할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c38d980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 1.1597 - val_loss: 153848.2812\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.8361 - val_loss: 19253.9766\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4630 - val_loss: 72827.2031\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4421 - val_loss: 40321.3750\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4318 - val_loss: 60312.2773\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4236 - val_loss: 55164.6133\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4150 - val_loss: 74109.3438\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4088 - val_loss: 82298.5000\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4054 - val_loss: 50631.8281\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3994 - val_loss: 90424.9531\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3967 - val_loss: 114505.2031\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3913 - val_loss: 85884.6406\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4035 - val_loss: 87759.2578\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3953 - val_loss: 71347.3359\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3842 - val_loss: 77832.7734\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3802 - val_loss: 106340.4062\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3763 - val_loss: 97173.3438\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3719 - val_loss: 54409.1367\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3697 - val_loss: 62635.2422\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3672 - val_loss: 45284.5117\n",
      "1/1 [==============================] - 0s 103ms/step\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                   validation_data=(X_valid, y_valid))\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dbe493",
   "metadata": {},
   "source": [
    "### 함수형 API를 사용해 복잡한 모델 만들기\n",
    "Sequential 모델은 쉽지만 입력과 출력이 여러 개거나 더 복잡한 네트워크 토폴로지를 갖는 신경망을 만들어야할 때가 있다.\n",
    "\n",
    "이럴때는 <b>함수형 API</b>를 사용한다. "
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "0db4e3e2",
   "metadata": {},
   "source": [
    "- <b> 와이드&딥 신경망\n",
    "    ![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e325288b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input 객체 만들기. 이 객체는 shape와 dtype을 포함해서 모델의 입력을 정의한다. 한 모델은 여러 입력을 가질수 있다.\n",
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "\n",
    "# 30개의 뉴런을 가지고 활성화함수로는 ReLU를 가진 Dense층을 만든다. 이 층은 만들어지자마자 입력과 함께 함수처럼 호출된다.\n",
    "# 케라스에 층이 연결될 방법을 알려주었을 뿐 아직 어떤 데이터도 처리하지 않았다.\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
    "\n",
    "# 두번째 은닉층을 만든다. 이 층은 hidden1과 연결되어있다.\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "\n",
    "# Concatenate 층을 만들고 또 다시 함수처럼 호출하여 두 번째 은닉층의 출려과 입력을 연결한다. \n",
    "# 아래 함수는 Concatenate 층을 만들고 주어진 입력으로 바로 호출한다.\n",
    "concat = keras.layers.Concatenate()([input_, hidden2])\n",
    "\n",
    "# 하나의 뉴런과 활성화 함수가 없는 출력층을 만들고 Concatenate 층이 만든 결과를 사용해 호출한다.\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "\n",
    "# 마지막으로 사용할 입력과 출력을 지정하여 케라스 Model을 만든다.\n",
    "model_wide_deep = keras.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e68c9f",
   "metadata": {},
   "source": [
    "- <b> 일부 특성은 짧은 경로로 전달하고 다른 특성들은 깊은 경로로 전달하고 싶다면 어떻게 해야할까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0fa1b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name=\"wide input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep input\")\n",
    "\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
    "model_two_input_wide_deep = keras.Model(inputs=[input_A, input_B], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f67dedea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 2.0458 - val_loss: 23159.0430\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.8263 - val_loss: 170099.2656\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.7071 - val_loss: 177741.2500\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.6534 - val_loss: 158759.6562\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.6161 - val_loss: 129512.7031\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5891 - val_loss: 102645.5859\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5692 - val_loss: 84712.2578\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5541 - val_loss: 71169.1250\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5433 - val_loss: 61333.6250\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.5341 - val_loss: 51051.4102\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5264 - val_loss: 43885.4961\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5211 - val_loss: 37952.3008\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5152 - val_loss: 35440.3711\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.5105 - val_loss: 33540.6484\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.5063 - val_loss: 33860.8906\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5015 - val_loss: 30071.1270\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4985 - val_loss: 30564.4844\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4949 - val_loss: 26689.8301\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4920 - val_loss: 25729.8184\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4886 - val_loss: 28221.2051\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.4559\n",
      "1/1 [==============================] - 0s 333ms/step\n"
     ]
    }
   ],
   "source": [
    "model_two_input_wide_deep.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "\n",
    "# 피처가 중복되도 괜찮다.\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:,:5], X_valid[:,2:]\n",
    "X_test_A, X_test_B = X_test[:,:5], X_test[:,2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "history = model_two_input_wide_deep.fit((X_train_A, X_train_B), y_train, epochs=20,\n",
    "                   validation_data= ((X_valid_A, X_valid_B), y_valid))\n",
    "\n",
    "mse_test = model_two_input_wide_deep.evaluate((X_test_A, X_test_B), y_test)\n",
    "y_pred = model_two_input_wide_deep.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a842cbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " deep input (InputLayer)        [(None, 6)]          0           []                               \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 30)           210         ['deep input[0][0]']             \n",
      "                                                                                                  \n",
      " wide input (InputLayer)        [(None, 5)]          0           []                               \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 30)           930         ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 35)           0           ['wide input[0][0]',             \n",
      "                                                                  'dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 1)            36          ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,176\n",
      "Trainable params: 1,176\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_two_input_wide_deep.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dc6694",
   "metadata": {},
   "source": [
    "- <b> 여러개의 출력이 필요한 경우</b>\n",
    "\n",
    "    ex) \n",
    "    1. 그림에 있는 주요 물체를 분류하고 위치를 알아야 하는 경우. (회귀작업 + 분류작업)\n",
    "    2. 동일한 데이터에서 독립적인 여러 작업을 수행할 때. (다중 작업 분류)\n",
    "    3. 규제기법을 사용하는 경우. 하위 네트워크가 나머지 네트워크에 의존하지 않고 그 자체로 유용한지를 판단할 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "729ab40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 4s 6ms/step - loss: 0.9897 - main_output_loss: 0.8777 - aux_output_loss: 1.9980 - val_loss: 52730.6016 - val_main_output_loss: 30485.6211 - val_aux_output_loss: 252935.3125\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.6853 - main_output_loss: 0.6422 - aux_output_loss: 1.0726 - val_loss: 23022.1328 - val_main_output_loss: 9170.2549 - val_aux_output_loss: 147688.9531\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.5837 - main_output_loss: 0.5465 - aux_output_loss: 0.9181 - val_loss: 27494.3398 - val_main_output_loss: 20054.1270 - val_aux_output_loss: 94456.1719\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.5384 - main_output_loss: 0.5087 - aux_output_loss: 0.8053 - val_loss: 23686.7637 - val_main_output_loss: 15905.2549 - val_aux_output_loss: 93720.3750\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.5134 - main_output_loss: 0.4858 - aux_output_loss: 0.7615 - val_loss: 16632.1035 - val_main_output_loss: 11024.4229 - val_aux_output_loss: 67101.2422\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.5049 - main_output_loss: 0.4810 - aux_output_loss: 0.7207 - val_loss: 15756.3145 - val_main_output_loss: 10467.4854 - val_aux_output_loss: 63355.7617\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4881 - main_output_loss: 0.4655 - aux_output_loss: 0.6916 - val_loss: 19588.2949 - val_main_output_loss: 15488.7764 - val_aux_output_loss: 56484.0430\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4720 - main_output_loss: 0.4504 - aux_output_loss: 0.6658 - val_loss: 12317.5771 - val_main_output_loss: 8832.5439 - val_aux_output_loss: 43682.8555\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4659 - main_output_loss: 0.4456 - aux_output_loss: 0.6493 - val_loss: 15805.2969 - val_main_output_loss: 12942.4873 - val_aux_output_loss: 41570.5703\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4603 - main_output_loss: 0.4411 - aux_output_loss: 0.6338 - val_loss: 26992.6816 - val_main_output_loss: 24485.9531 - val_aux_output_loss: 49553.2578\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4501 - main_output_loss: 0.4316 - aux_output_loss: 0.6170 - val_loss: 16321.3477 - val_main_output_loss: 13626.0508 - val_aux_output_loss: 40579.0156\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4410 - main_output_loss: 0.4224 - aux_output_loss: 0.6081 - val_loss: 17021.5996 - val_main_output_loss: 14281.3223 - val_aux_output_loss: 41684.0898\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4401 - main_output_loss: 0.4225 - aux_output_loss: 0.5984 - val_loss: 10177.6523 - val_main_output_loss: 8202.1025 - val_aux_output_loss: 27957.6035\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4303 - main_output_loss: 0.4136 - aux_output_loss: 0.5808 - val_loss: 11164.1865 - val_main_output_loss: 9658.3018 - val_aux_output_loss: 24717.1680\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4223 - main_output_loss: 0.4057 - aux_output_loss: 0.5718 - val_loss: 6461.2476 - val_main_output_loss: 4706.1230 - val_aux_output_loss: 22257.3750\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4399 - main_output_loss: 0.4226 - aux_output_loss: 0.5952 - val_loss: 8465.5381 - val_main_output_loss: 7001.2178 - val_aux_output_loss: 21644.4043\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4357 - main_output_loss: 0.4185 - aux_output_loss: 0.5903 - val_loss: 6951.1729 - val_main_output_loss: 4779.8081 - val_aux_output_loss: 26493.4609\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4217 - main_output_loss: 0.4069 - aux_output_loss: 0.5548 - val_loss: 14196.0215 - val_main_output_loss: 13819.5918 - val_aux_output_loss: 17583.8672\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4109 - main_output_loss: 0.3958 - aux_output_loss: 0.5472 - val_loss: 7080.9155 - val_main_output_loss: 6503.6650 - val_aux_output_loss: 12276.1924\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4056 - main_output_loss: 0.3909 - aux_output_loss: 0.5373 - val_loss: 9726.9980 - val_main_output_loss: 8696.7588 - val_aux_output_loss: 18999.1465\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.3628 - main_output_loss: 0.3488 - aux_output_loss: 0.4885\n",
      "1/1 [==============================] - 0s 69ms/step\n"
     ]
    }
   ],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name=\"wide input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep input\")\n",
    "\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_A, hidden2])\n",
    "main_output = keras.layers.Dense(1, name=\"main_output\")(concat)\n",
    "aux_output = keras.layers.Dense(1, name=\"aux_output\")(hidden2)\n",
    "\n",
    "model_two_output_wide_deep = keras.Model(inputs=[input_A, input_B], outputs=[main_output, aux_output])\n",
    "\n",
    "model_two_output_wide_deep.compile(loss=[\"mse\", \"mse\"], optimizer=\"sgd\", loss_weights = [0.9, 0.1])\n",
    "\n",
    "# 피처가 중복되도 괜찮다.\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:,:5], X_valid[:,2:]\n",
    "X_test_A, X_test_B = X_test[:,:5], X_test[:,2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "history = model_two_output_wide_deep.fit((X_train_A, X_train_B), [y_train,y_train], epochs=20,\n",
    "                   validation_data= ((X_valid_A, X_valid_B), (y_valid, y_valid)))\n",
    "\n",
    "mse_test = model_two_output_wide_deep.evaluate((X_test_A, X_test_B), y_test)\n",
    "y_pred_main, y_pred_aux = model_two_output_wide_deep.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c1cc387b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 3ms/step - loss: 0.3628 - main_output_loss: 0.3488 - aux_output_loss: 0.4885\n"
     ]
    }
   ],
   "source": [
    "total_loss, main_loss ,aux_loss = model_two_output_wide_deep.evaluate([X_test_A, X_test_B], [y_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "273b6871",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dynamic_WideAndDeepModel(keras.Model):\n",
    "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs) # 표준 매개변수를 처리한다. ex) name\n",
    "        self.hidden1 = keras.layers.Dense(units, activation= activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation= activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        input_A , input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input1, input2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "    \n",
    "dynamic_wide_deep_model = Dynamic_WideAndDeepModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b8bbeb",
   "metadata": {},
   "source": [
    "이 예제는 함수형 API와 굉장히 비슷하지만, call함수에 무수히 많은 계산들을 넣을 수도 있다. 자유자재로 변형이 가능하다는 것이다.\n",
    "\n",
    "하지만 call함수에 층간의 관계가 있으므로 모델을 저장하거나 복사할 수가 없다는 불편함이 존재한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c0e04d",
   "metadata": {},
   "source": [
    "### 모델 저장과 복원\n",
    "시퀀셜 API나 함수형 API를 사용하면 모델을 저장하는 것은 다음과 같이 매수 쉽다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20475089",
   "metadata": {},
   "source": [
    "``` python\n",
    "## 모델 저장\n",
    "model = keras.models.Sequential([...])\n",
    "model.compile([...])\n",
    "model.fit([...])\n",
    "model.save(\"my_keras_model.h5\")\n",
    "\n",
    "## 모델 불러오기\n",
    "model = keras.models.load_model(\"my_keras_model.h5\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cbf7ee",
   "metadata": {},
   "source": [
    "## 콜백 사용하기\n",
    "fit 메소드에서 체크포인트를 저장하는 방법"
   ]
  },
  {
   "cell_type": "raw",
   "id": "774cf933",
   "metadata": {},
   "source": [
    "# 간단한 체크포인트 저장\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckPoint(\"my_keras_model.h5\")\n",
    "history = model.fit(X_train, y_train, epochs=10, callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "22a01cfa",
   "metadata": {},
   "source": [
    "# 최상의 모델만 저장하는 callbacks 메소드 사용법\n",
    "# 조기종료를 구현하는 것임\n",
    "checkpoint_cb = keras.callbacks.ModelCheckPoint(\"my_keras_model.h5\",\n",
    "                                               save_best_only=True)\n",
    "history = model.fit(X_train, y_train, epochs=10, callbacks=[checkpoint_cb])\n",
    "\n",
    "model = keras.model.load_model(\"my_keras_model.h5\") # 최상의 모델로 복원"
   ]
  },
  {
   "cell_type": "raw",
   "id": "55ff7c7b",
   "metadata": {},
   "source": [
    "# 조기종료를 구현하는 방법\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,\n",
    "                                                 restore_best_weight=True)\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                   validation_data=(X_valid, y_valid),\n",
    "                   callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106ad312",
   "metadata": {},
   "source": [
    "### 텐서보드를 사용해서 시각화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb5aa8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "23f9ad55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3637 - val_loss: 53801.4961\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3629 - val_loss: 27846.7598\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3622 - val_loss: 34074.1211\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3587 - val_loss: 28352.1562\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3560 - val_loss: 40420.8750\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3562 - val_loss: 44642.9961\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3522 - val_loss: 70705.3125\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3502 - val_loss: 51822.0352\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3488 - val_loss: 27289.7949\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3565 - val_loss: 28899.6191\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3479 - val_loss: 59179.4453\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3452 - val_loss: 62973.7539\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3432 - val_loss: 44809.3203\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3415 - val_loss: 58504.3828\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3398 - val_loss: 52159.1680\n",
      "Epoch 16/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3387 - val_loss: 51194.6133\n",
      "Epoch 17/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3410 - val_loss: 42015.3672\n",
      "Epoch 18/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3382 - val_loss: 60589.4570\n",
      "Epoch 19/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3369 - val_loss: 52889.5703\n",
      "Epoch 20/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3360 - val_loss: 39743.0469\n",
      "Epoch 21/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3368 - val_loss: 44446.1836\n",
      "Epoch 22/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3340 - val_loss: 38647.7695\n",
      "Epoch 23/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3348 - val_loss: 68149.8516\n",
      "Epoch 24/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3331 - val_loss: 37388.0586\n",
      "Epoch 25/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3314 - val_loss: 33523.4414\n",
      "Epoch 26/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3321 - val_loss: 89337.3906\n",
      "Epoch 27/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3305 - val_loss: 39693.6562\n",
      "Epoch 28/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3327 - val_loss: 87049.3125\n",
      "Epoch 29/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3317 - val_loss: 32333.0742\n",
      "Epoch 30/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3298 - val_loss: 42891.7383\n"
     ]
    }
   ],
   "source": [
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                   validation_data=(X_valid, y_valid),\n",
    "                   callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e681f6a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 17440), started 3 days, 23:07:48 ago. (Use '!kill 17440' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-35a7223be45a6497\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-35a7223be45a6497\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 주피터 노트북 내부에서 텐서보드 사용\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331c47bf",
   "metadata": {},
   "source": [
    "### 신경망 하이퍼파라미터 튜닝하기\n",
    "그리드서치 or RandomizedSearchCV를 사용하기 위해서는 케라스 모델을 사이킷런 추정기처럼 보이도록 해야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf694199",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_12716/3126652774.py:11: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_reg = keras.wrappers.scikit_learn.KerasRegressor(bulid_model)\n"
     ]
    }
   ],
   "source": [
    "def bulid_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayers(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(bulid_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947cd2ac",
   "metadata": {},
   "source": [
    "위와 같은 오류가 떴다. 그래서 scikeras를 설치했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e589cbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d65f6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "\n",
    "def bulid_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "keras_reg = scikeras.wrappers.KerasRegressor(bulid_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801f5abb",
   "metadata": {},
   "source": [
    "사이킷런 추정기처럼 보이게 하는 래퍼를 감싸면 사이킷런에서 사용되는 메소드를 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c1a68f70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 1.5383 - val_loss: 100063.3438\n",
      "Epoch 2/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.7809 - val_loss: 113323.7109\n",
      "Epoch 3/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.6223 - val_loss: 63210.6211\n",
      "Epoch 4/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5673 - val_loss: 89869.0156\n",
      "Epoch 5/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5348 - val_loss: 61910.8594\n",
      "Epoch 6/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5124 - val_loss: 44106.4922\n",
      "Epoch 7/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5005 - val_loss: 38721.6602\n",
      "Epoch 8/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4917 - val_loss: 29073.8086\n",
      "Epoch 9/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4848 - val_loss: 28429.7500\n",
      "Epoch 10/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4784 - val_loss: 24399.7676\n",
      "Epoch 11/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4728 - val_loss: 30772.4336\n",
      "Epoch 12/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4688 - val_loss: 20830.1094\n",
      "Epoch 13/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4646 - val_loss: 28682.2227\n",
      "Epoch 14/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4612 - val_loss: 23858.8535\n",
      "Epoch 15/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4574 - val_loss: 21225.7070\n",
      "Epoch 16/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4558 - val_loss: 17352.3867\n",
      "Epoch 17/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4544 - val_loss: 13942.7734\n",
      "Epoch 18/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4496 - val_loss: 16115.6895\n",
      "Epoch 19/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4468 - val_loss: 16365.9072\n",
      "Epoch 20/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4448 - val_loss: 14620.6572\n",
      "Epoch 21/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4423 - val_loss: 15221.4053\n",
      "Epoch 22/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4402 - val_loss: 17524.0215\n",
      "Epoch 23/50\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4383 - val_loss: 20436.1406\n",
      "Epoch 24/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4362 - val_loss: 17211.5273\n",
      "Epoch 25/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4347 - val_loss: 16623.7402\n",
      "Epoch 26/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4322 - val_loss: 16188.7119\n",
      "Epoch 27/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4312 - val_loss: 20095.2461\n",
      "Epoch 28/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4286 - val_loss: 18477.9551\n",
      "Epoch 29/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4273 - val_loss: 19401.5156\n",
      "Epoch 30/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4255 - val_loss: 22061.9883\n",
      "Epoch 31/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4234 - val_loss: 20381.5820\n",
      "Epoch 32/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4226 - val_loss: 16292.7051\n",
      "Epoch 33/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4222 - val_loss: 26374.3711\n",
      "Epoch 34/50\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4199 - val_loss: 22145.8359\n",
      "Epoch 35/50\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4194 - val_loss: 26523.0312\n",
      "Epoch 36/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4170 - val_loss: 22153.1582\n",
      "Epoch 37/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4157 - val_loss: 21012.2773\n",
      "Epoch 38/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4133 - val_loss: 22638.6680\n",
      "Epoch 39/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4218 - val_loss: 20419.2969\n",
      "Epoch 40/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4253 - val_loss: 18137.0430\n",
      "Epoch 41/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4110 - val_loss: 25617.5898\n",
      "Epoch 42/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4098 - val_loss: 24393.5430\n",
      "Epoch 43/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4065 - val_loss: 13207.9521\n",
      "Epoch 44/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4047 - val_loss: 22657.6992\n",
      "Epoch 45/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4029 - val_loss: 16383.0879\n",
      "Epoch 46/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4018 - val_loss: 19242.6172\n",
      "Epoch 47/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4013 - val_loss: 21285.4648\n",
      "Epoch 48/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4002 - val_loss: 22623.3398\n",
      "Epoch 49/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3983 - val_loss: 17931.7148\n",
      "Epoch 50/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3968 - val_loss: 18836.7363\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000020B924A7AF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "162/162 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n"
     ]
    }
   ],
   "source": [
    "keras_reg.fit(X_train, y_train, epochs=50,\n",
    "             validation_data=(X_valid, y_valid),\n",
    "             callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "mse_test = keras_reg.score(X_test, y_test)\n",
    "y_pred = keras_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dd981437",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter learning_rate for estimator KerasRegressor.\nThis issue can likely be resolved by setting this parameter in the KerasRegressor constructor:\n`KerasRegressor(learning_rate=0.007128700290480555)`\nCheck the list of available parameters with `estimator.get_params().keys()`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12716/2901470443.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mrnd_search_cv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras_reg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_distribs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m rnd_search_cv.fit(X_train, y_train, epochs=100,\n\u001b[0m\u001b[0;32m     14\u001b[0m                  \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    873\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 875\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    876\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    877\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1751\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1752\u001b[0m         \u001b[1;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1753\u001b[1;33m         evaluate_candidates(\n\u001b[0m\u001b[0;32m   1754\u001b[0m             ParameterSampler(\n\u001b[0;32m   1755\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    820\u001b[0m                     )\n\u001b[0;32m    821\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 822\u001b[1;33m                 out = parallel(\n\u001b[0m\u001b[0;32m    823\u001b[0m                     delayed(_fit_and_score)(\n\u001b[0;32m    824\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1043\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1044\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    860\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 861\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    862\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    672\u001b[0m             \u001b[0mcloned_parameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 674\u001b[1;33m         \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mcloned_parameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scikeras\\wrappers.py\u001b[0m in \u001b[0;36mset_params\u001b[1;34m(self, **params)\u001b[0m\n\u001b[0;32m   1166\u001b[0m                     \u001b[1;31m# Give a SciKeras specific user message to aid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1167\u001b[0m                     \u001b[1;31m# in moving from the Keras wrappers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1168\u001b[1;33m                     raise ValueError(\n\u001b[0m\u001b[0;32m   1169\u001b[0m                         \u001b[1;34mf\"Invalid parameter {param} for estimator {self.__name__}.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1170\u001b[0m                         \u001b[1;34m\"\\nThis issue can likely be resolved by setting this parameter\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid parameter learning_rate for estimator KerasRegressor.\nThis issue can likely be resolved by setting this parameter in the KerasRegressor constructor:\n`KerasRegressor(learning_rate=0.007128700290480555)`\nCheck the list of available parameters with `estimator.get_params().keys()`"
     ]
    }
   ],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "keras_reg.get_params().keys()\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_hidden\": [0,1,2,3],\n",
    "    \"n_neurons\": np.arange(1,100),\n",
    "    \"learning_rate\": reciprocal(3e-4, 3e-2),\n",
    "}\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=100,\n",
    "                 validation_data=(X_valid, y_valid),\n",
    "                 callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb14f94",
   "metadata": {},
   "source": [
    "으악 책에 나온대로 안하고 sci-keras를 사용하니까 오류가 뜸... 아래는 제대로 나온다ㅜㅜ 뭔지를 모르겠다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18981af4",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d53a9a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_12716/3730239423.py:11: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n"
     ]
    }
   ],
   "source": [
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3404cef3",
   "metadata": {},
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "param_distribs = {\n",
    "    'n_hidden': [2,3,4,5],\n",
    "    'n_neurons': np.arange(1,100),\n",
    "    'learning_rate': reciprocal(3e-4, 3e-2)\n",
    "}\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3, verbose=2)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=100,\n",
    "                 validation_data=(X_valid, y_valid),\n",
    "                 callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4c74dc3f",
   "metadata": {},
   "source": [
    "# 이렇게 하면 최적 파라미터가 나온다. 그런데 너너너너너너무무무무 학습을 오래해서 그냥 이걸로 대체함.\n",
    "rnd_search_cv.best_parmas_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2c5949",
   "metadata": {},
   "source": [
    "잘 모르겠는 부분이 있는데, 래퍼로 감싸면 scikit_learn형태일텐데 keras의 파라미터를 받아서 랜덤서치를 해도 되는건가??? \n",
    "scikeras를 사용했을 때는 오류가 떠서 의문이 들었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77328bee",
   "metadata": {},
   "source": [
    "### 연습문제 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6af0895",
   "metadata": {},
   "source": [
    "논리 연산에 대한 문제는 잘 모르겠다..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7277a7bb",
   "metadata": {},
   "source": [
    "### 연습문제 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97b588f",
   "metadata": {},
   "source": [
    "고전적인 퍼셉트론보다 로지스틱 회귀 분류가 선호되는 이유는 \n",
    "1. 클래스별 확률을 제공하지 않음 (왜냐하면 계단형식의 활성화함수를 사용하기 때문이다.)\n",
    "2. 데이터가 선형적으로 구분 될 때만 수렴한다. (논리연산과 관련이 있는 것 같은데 잘 모르겠다.)\n",
    "이를 해소하기 위해서는 활성화 함수를 로지스틱 활성화 함수로 바꾸고 경사하강법으로 훈련한다. 또한 다층 퍼셉트론을 사용하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffe5a7f",
   "metadata": {},
   "source": [
    "### 연습문제 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eae6992",
   "metadata": {},
   "source": [
    "경사가 없는 계단 함수는 경사하강법을 적용할 수 없기 때문에 경사가 있는 로지스틱 활성화 함수를 사용했다. \n",
    "또한, 실제 뉴런의 활성화함수 모양이 로지스틱 활성화 함수와 비슷하다고 생각했기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1df4b35",
   "metadata": {},
   "source": [
    "### 연습문제 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadcfd77",
   "metadata": {},
   "source": [
    "1. ReLU\n",
    "2. tanh 함수\n",
    "3. 시그모이드 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12bca4d",
   "metadata": {},
   "source": [
    "### 연습문제 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cdbc20",
   "metadata": {},
   "source": [
    "- 입력 행렬 X의 크기는 m * 10 이다. (m은 배치의 크기이다.)\n",
    "- 은닉층의 가중치 행렬(W_h)은 10 * 50 이다. 은닉층의 편향(b_h)행렬은 1 * 50 이다.\n",
    "- 출력층의 가중치 행렬(W_o)은 50 * 3 이다. 은닉층의 편향(b_o)행렬은 1 * 3 이다. \n",
    "- 네트워크 출력 행렬 Y의 크기는 m * 3이다. \n",
    "\n",
    "\n",
    "- 출력 행렬: Y = ReLU(ReLU(W_h\\*X+b_h)\\*W_o+b_o)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6971e1a",
   "metadata": {},
   "source": [
    "### 연습문제 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acb9888",
   "metadata": {},
   "source": [
    "~~스팸 구별문제는 출력층에 스팸/비스팸을 구별할 수 있도록 두개의 뉴런이 필요하다. 출력층 함수는 출력 결과가 서로 배타적이므로 소프트맥스함수가 적절할 것 같다.~~ 이 방법이 아닌 한개의 출력 뉴런을 가지고 활성화함수는 로지스틱 활성화 함수를 가지는 뉴런이여도 괜찮다.\n",
    "보통 이진 분류에는 로지스틱 활성화 함수가 사용된다. \n",
    "\n",
    "\n",
    "Mnist 구별문제는 출력층에 10개의 뉴런이 필요하다. 또한 소프트 맥스함수를 사용하면 될 것 같다.\n",
    "\n",
    "회귀문제는 출력에 1개의 뉴런이 필요하다. 이건 활성화 함수가 필요 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6724d700",
   "metadata": {},
   "source": [
    "### 연습문제 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68fc8fc",
   "metadata": {},
   "source": [
    "역전파는 여러 층으로 이루어져 있는 모델의 뉴런들을 학습하는 방법이다. 여기에는 경사하강법이 이용되는데 이때 그래디언트를 계산하기 위한 방법이 후진 모드 자동 미분이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf014bd",
   "metadata": {},
   "source": [
    "### 연습문제 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25ffd22",
   "metadata": {},
   "source": [
    "다층 퍼셉트론에서 조정할 수 있는 파라미터는\n",
    "- 뉴런 개수\n",
    "- 은닉층 수\n",
    "- 학습률\n",
    "- 옵티마이저\n",
    "- 활성화 함수\n",
    "가 있다. \n",
    "과대 적합이 발생하면 뉴런개수를 줄이고, 은닉층 수를 줄이는 방법이 있고, \n",
    "규제를 통해서 과대적합을 해소할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e5a50a",
   "metadata": {},
   "source": [
    "### 연습문제 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8e3161",
   "metadata": {},
   "source": [
    "#### 0. 모듈 import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab35a094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76d0a94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear_session\n",
    "# keras.backend.clear_session()\n",
    "# 현재 TF 그래프를 없애고, 새로운 TF 그래프를 만듭니다.\n",
    "#오래된 모델 혹은 층과의 혼란을 피할 때 유용합니다.\n",
    "keras.backend.clear_session()\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61555857",
   "metadata": {},
   "source": [
    "#### 1. 데이터 적재/ test_train셋 분리/ 스케일링/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02d4c808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "mnist_data = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50e573b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train 크기:  (55000, 28, 28)\n",
      "X_valid 크기:  (5000, 28, 28)\n",
      "y_train 크기:  (55000,)\n",
      "y_valid 크기:  (5000,)\n"
     ]
    }
   ],
   "source": [
    "train, test= mnist_data\n",
    "X_train_full, y_train_full, X_test, y_test = train[0], train[1], test[0], test[1]\n",
    "\n",
    "# 검증셋을 20%로 하니까 accuracy가 너무 낮았다. 그래서 작게했다. \n",
    "X_train, X_valid = X_train_full[5000:], X_train_full[:5000]\n",
    "y_train, y_valid = y_train_full[5000:], y_train_full[:5000]\n",
    "\n",
    "# mnist 데이터는 0~255의 범위를 가지므로 0~1 사이의 범위를 갖도록 \n",
    "X_train, X_valid, X_test = X_train/255.0, X_valid/255.0, X_test/255.0\n",
    "\n",
    "\n",
    "print(\"X_train 크기: \", X_train.shape)\n",
    "print(\"X_valid 크기: \", X_valid.shape)\n",
    "print(\"y_train 크기: \", y_train.shape)\n",
    "print(\"y_valid 크기: \", y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0a5273",
   "metadata": {},
   "source": [
    "#### 2. 모델 구현2. 모델 구현<br>\n",
    "- 시퀀셜 api 사용 \n",
    "\n",
    "A. 입력층에 784개의 뉴런이 필요함<br>\n",
    "B. 첫번째 은닉층에 300개의 뉴런 (활성화 함수 ReLU)<br>\n",
    "C. 두번째 은닉층에 100개의 뉴런 (활성화 함수 ReLU)<br>\n",
    "D. 출력층에 10개의 뉴런을 사용한다. (활성화 함수 softmax)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "40301245",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_my = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dense(300, activation='relu'),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f6959a",
   "metadata": {},
   "source": [
    "#### 3. 모델 compile<br>\n",
    "\n",
    "A. 학습률 (학습률을 지수적으로 증가시키면서 찾기)<br>\n",
    "B. 옵티마이저   <br>\n",
    "C. 손실함수 (분류기 때문에 카테고리 엔트로피 함수 사용하면 될 듯)<br>\n",
    "D. 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4109b14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_my.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "             optimizer=\"sgd\",\n",
    "             metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87dde8b",
   "metadata": {},
   "source": [
    "#### 4. 모델 fit <br>\n",
    "A. X_train, y_train<br>\n",
    "B. 조기종료 (X_valid, y_valid)<br>\n",
    "C. 텐서 보드 사용하기<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5356dea4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.6301 - accuracy: 0.8345 - val_loss: 0.3080 - val_accuracy: 0.9166\n",
      "Epoch 2/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.2878 - accuracy: 0.9184 - val_loss: 0.2388 - val_accuracy: 0.9330\n",
      "Epoch 3/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.2329 - accuracy: 0.9336 - val_loss: 0.1979 - val_accuracy: 0.9444\n",
      "Epoch 4/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.1971 - accuracy: 0.9440 - val_loss: 0.1739 - val_accuracy: 0.9546\n",
      "Epoch 5/100\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.1716 - accuracy: 0.9509 - val_loss: 0.1543 - val_accuracy: 0.9584\n",
      "Epoch 6/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.1520 - accuracy: 0.9570 - val_loss: 0.1415 - val_accuracy: 0.9630\n",
      "Epoch 7/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.1363 - accuracy: 0.9623 - val_loss: 0.1304 - val_accuracy: 0.9642\n",
      "Epoch 8/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.1235 - accuracy: 0.9645 - val_loss: 0.1261 - val_accuracy: 0.9666\n",
      "Epoch 9/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.1128 - accuracy: 0.9683 - val_loss: 0.1151 - val_accuracy: 0.9690\n",
      "Epoch 10/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.1041 - accuracy: 0.9706 - val_loss: 0.1065 - val_accuracy: 0.9710\n",
      "Epoch 11/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0958 - accuracy: 0.9733 - val_loss: 0.1047 - val_accuracy: 0.9722\n",
      "Epoch 12/100\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.0887 - accuracy: 0.9751 - val_loss: 0.1004 - val_accuracy: 0.9712\n",
      "Epoch 13/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0825 - accuracy: 0.9767 - val_loss: 0.0937 - val_accuracy: 0.9742\n",
      "Epoch 14/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0768 - accuracy: 0.9784 - val_loss: 0.0923 - val_accuracy: 0.9742\n",
      "Epoch 15/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0717 - accuracy: 0.9802 - val_loss: 0.0878 - val_accuracy: 0.9760\n",
      "Epoch 16/100\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.0671 - accuracy: 0.9808 - val_loss: 0.0863 - val_accuracy: 0.9772\n",
      "Epoch 17/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0628 - accuracy: 0.9826 - val_loss: 0.0858 - val_accuracy: 0.9762\n",
      "Epoch 18/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0592 - accuracy: 0.9838 - val_loss: 0.0823 - val_accuracy: 0.9770\n",
      "Epoch 19/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0555 - accuracy: 0.9849 - val_loss: 0.0809 - val_accuracy: 0.9768\n",
      "Epoch 20/100\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.0523 - accuracy: 0.9853 - val_loss: 0.0805 - val_accuracy: 0.9764\n",
      "Epoch 21/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0494 - accuracy: 0.9867 - val_loss: 0.0769 - val_accuracy: 0.9790\n",
      "Epoch 22/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0465 - accuracy: 0.9871 - val_loss: 0.0765 - val_accuracy: 0.9800\n",
      "Epoch 23/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0438 - accuracy: 0.9884 - val_loss: 0.0796 - val_accuracy: 0.9766\n",
      "Epoch 24/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0411 - accuracy: 0.9895 - val_loss: 0.0758 - val_accuracy: 0.9788\n",
      "Epoch 25/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0392 - accuracy: 0.9899 - val_loss: 0.0737 - val_accuracy: 0.9796\n",
      "Epoch 26/100\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.0368 - accuracy: 0.9904 - val_loss: 0.0715 - val_accuracy: 0.9798\n",
      "Epoch 27/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0349 - accuracy: 0.9912 - val_loss: 0.0712 - val_accuracy: 0.9810\n",
      "Epoch 28/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0329 - accuracy: 0.9919 - val_loss: 0.0739 - val_accuracy: 0.9798\n",
      "Epoch 29/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0309 - accuracy: 0.9924 - val_loss: 0.0730 - val_accuracy: 0.9796\n",
      "Epoch 30/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0292 - accuracy: 0.9931 - val_loss: 0.0714 - val_accuracy: 0.9804\n",
      "Epoch 31/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0276 - accuracy: 0.9935 - val_loss: 0.0719 - val_accuracy: 0.9800\n",
      "Epoch 32/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0260 - accuracy: 0.9940 - val_loss: 0.0710 - val_accuracy: 0.9810\n",
      "Epoch 33/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0247 - accuracy: 0.9946 - val_loss: 0.0717 - val_accuracy: 0.9800\n",
      "Epoch 34/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0235 - accuracy: 0.9951 - val_loss: 0.0729 - val_accuracy: 0.9802\n",
      "Epoch 35/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0222 - accuracy: 0.9955 - val_loss: 0.0706 - val_accuracy: 0.9814\n",
      "Epoch 36/100\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.0208 - accuracy: 0.9960 - val_loss: 0.0730 - val_accuracy: 0.9800\n",
      "Epoch 37/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0198 - accuracy: 0.9964 - val_loss: 0.0685 - val_accuracy: 0.9816\n",
      "Epoch 38/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0188 - accuracy: 0.9967 - val_loss: 0.0706 - val_accuracy: 0.9810\n",
      "Epoch 39/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0179 - accuracy: 0.9971 - val_loss: 0.0707 - val_accuracy: 0.9812\n",
      "Epoch 40/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0169 - accuracy: 0.9973 - val_loss: 0.0720 - val_accuracy: 0.9814\n",
      "Epoch 41/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0161 - accuracy: 0.9976 - val_loss: 0.0700 - val_accuracy: 0.9820\n",
      "Epoch 42/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0152 - accuracy: 0.9978 - val_loss: 0.0698 - val_accuracy: 0.9816\n",
      "Epoch 43/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0145 - accuracy: 0.9980 - val_loss: 0.0739 - val_accuracy: 0.9790\n",
      "Epoch 44/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0138 - accuracy: 0.9982 - val_loss: 0.0701 - val_accuracy: 0.9812\n",
      "Epoch 45/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0130 - accuracy: 0.9985 - val_loss: 0.0712 - val_accuracy: 0.9808\n",
      "Epoch 46/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0126 - accuracy: 0.9985 - val_loss: 0.0710 - val_accuracy: 0.9818\n",
      "Epoch 47/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0119 - accuracy: 0.9987 - val_loss: 0.0709 - val_accuracy: 0.9820\n",
      "Epoch 48/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0112 - accuracy: 0.9987 - val_loss: 0.0732 - val_accuracy: 0.9810\n",
      "Epoch 49/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0107 - accuracy: 0.9989 - val_loss: 0.0708 - val_accuracy: 0.9828\n",
      "Epoch 50/100\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.0103 - accuracy: 0.9990 - val_loss: 0.0722 - val_accuracy: 0.9812\n",
      "Epoch 51/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0098 - accuracy: 0.9991 - val_loss: 0.0721 - val_accuracy: 0.9824\n",
      "Epoch 52/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0095 - accuracy: 0.9992 - val_loss: 0.0714 - val_accuracy: 0.9826\n",
      "Epoch 53/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0091 - accuracy: 0.9992 - val_loss: 0.0724 - val_accuracy: 0.9814\n",
      "Epoch 54/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0087 - accuracy: 0.9992 - val_loss: 0.0726 - val_accuracy: 0.9818\n",
      "Epoch 55/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0084 - accuracy: 0.9994 - val_loss: 0.0742 - val_accuracy: 0.9822\n",
      "Epoch 56/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0079 - accuracy: 0.9995 - val_loss: 0.0752 - val_accuracy: 0.9816\n",
      "Epoch 57/100\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.0077 - accuracy: 0.9993 - val_loss: 0.0727 - val_accuracy: 0.9822\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()\n",
    "\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "\n",
    "history = model_my.fit(X_train, y_train, epochs=100,\n",
    "                   validation_data=(X_valid, y_valid),\n",
    "                   callbacks= [early_stopping_cb, tensorboard_cb])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "78c6943a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 17440), started 3 days, 23:19:41 ago. (Use '!kill 17440' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c54069a371a5060c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c54069a371a5060c\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "86e6f0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0714 - accuracy: 0.9795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0713987722992897, 0.9794999957084656]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_my.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bbc9e8",
   "metadata": {},
   "source": [
    "### 학습률 튜닝 \n",
    "이 아래 코드는 잘 모르겠어서 답안을 참조했습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33b94a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 잘은 모르겠지만 이해한 내용을 써본다.\n",
    "# keras 는 low-level에서의 조정은 안된다. 왜냐하면 쉽게 사용하기 위한 딥러닝 api이기 때문이다. \n",
    "# 하지만 keras.backend를 사용하면 low-level에서의 조정이 가능하다.\n",
    "## set_value\n",
    "## keras.backend.set_value(x, value) 로 사용한다. \n",
    "#  반복마다 학습률을 증가시키기 위해 콜백을 사용합니다. 이 콜백은 반복마다 학습률과 손실을 기록합니다.\n",
    "K = keras.backend\n",
    "\n",
    "# 학습률을 지수함수적으로 증가시키면서 성능을 보는 콜백함수를 만들 클래스를 만든 것?\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "        \n",
    "        \n",
    "    # 파라미터로 들어간 batch는 잘 모르겠다. logs 는 모델이 학습된 기록들을 가져오는 것 같다.\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        # 현재 학습 모델의 학습률을 가져와서 리스트에 추가\n",
    "        self.rates.append(K.get_value(self.model.optimizer.lr))\n",
    "        # 현재 학습 모델의 손실을 리스트에 추가\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        # 학습률에 self.factor을 곱한값을 학습률로 재설정\n",
    "        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46760b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear_session\n",
    "# keras.backend.clear_session()\n",
    "# 현재 TF 그래프를 없애고, 새로운 TF 그래프를 만듭니다.\n",
    "#오래된 모델 혹은 층과의 혼란을 피할 때 유용합니다.\n",
    "keras.backend.clear_session()\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d57c6b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1/1719 [..............................] - ETA: 4:52 - loss: 2.3870 - accuracy: 0.0625WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0006s vs `on_train_batch_end` time: 0.0007s). Check your callbacks.\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: nan - accuracy: 0.6270 - val_loss: nan - val_accuracy: 0.0958\n"
     ]
    }
   ],
   "source": [
    "model_anwser = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dense(300, activation='relu'),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model_anwser.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "expon_lr = ExponentialLearningRate(factor=1.005)\n",
    "\n",
    "history = model_anwser.fit(X_train, y_train, epochs=1,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[expon_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b7f0999",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(expon_lr.rates, expon_lr.losses)\n",
    "plt.gca().set_xscale('log')\n",
    "plt.hlines(min(expon_lr.losses), min(expon_lr.rates), max(expon_lr.rates))\n",
    "plt.axis([min(expon_lr.rates), max(expon_lr.rates), 0, expon_lr.losses[0]])\n",
    "plt.grid()\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41045e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.387019634246826,\n",
       " 2.376983165740967,\n",
       " 2.3636763095855713,\n",
       " 2.37388277053833,\n",
       " 2.3640472888946533,\n",
       " 2.3509280681610107,\n",
       " 2.3422930240631104,\n",
       " 2.340693712234497,\n",
       " 2.3402111530303955,\n",
       " 2.3343379497528076,\n",
       " 2.3327927589416504,\n",
       " 2.334876298904419,\n",
       " 2.3315837383270264,\n",
       " 2.3315017223358154,\n",
       " 2.333292245864868,\n",
       " 2.333632469177246,\n",
       " 2.3276474475860596,\n",
       " 2.3256337642669678,\n",
       " 2.3249316215515137,\n",
       " 2.3227503299713135,\n",
       " 2.3226845264434814,\n",
       " 2.320162534713745,\n",
       " 2.3219406604766846,\n",
       " 2.3205840587615967,\n",
       " 2.319373369216919,\n",
       " 2.3155317306518555,\n",
       " 2.3164124488830566,\n",
       " 2.3154542446136475,\n",
       " 2.314448595046997,\n",
       " 2.3127732276916504,\n",
       " 2.31126070022583,\n",
       " 2.3113842010498047,\n",
       " 2.3105456829071045,\n",
       " 2.309812545776367,\n",
       " 2.3082058429718018,\n",
       " 2.307255983352661,\n",
       " 2.3075315952301025,\n",
       " 2.3074233531951904,\n",
       " 2.306504249572754,\n",
       " 2.3061470985412598,\n",
       " 2.305089235305786,\n",
       " 2.304429531097412,\n",
       " 2.3048253059387207,\n",
       " 2.304718255996704,\n",
       " 2.303907871246338,\n",
       " 2.3048267364501953,\n",
       " 2.3046836853027344,\n",
       " 2.304267406463623,\n",
       " 2.3054850101470947,\n",
       " 2.3049004077911377,\n",
       " 2.304621458053589,\n",
       " 2.304265022277832,\n",
       " 2.3048624992370605,\n",
       " 2.3031978607177734,\n",
       " 2.303462028503418,\n",
       " 2.303159713745117,\n",
       " 2.302614688873291,\n",
       " 2.3017239570617676,\n",
       " 2.3005218505859375,\n",
       " 2.299891233444214,\n",
       " 2.299255609512329,\n",
       " 2.2998995780944824,\n",
       " 2.298492431640625,\n",
       " 2.297592878341675,\n",
       " 2.29679799079895,\n",
       " 2.2954764366149902,\n",
       " 2.293911933898926,\n",
       " 2.2937541007995605,\n",
       " 2.2929000854492188,\n",
       " 2.292611598968506,\n",
       " 2.291717767715454,\n",
       " 2.2905771732330322,\n",
       " 2.2905683517456055,\n",
       " 2.290581464767456,\n",
       " 2.2904374599456787,\n",
       " 2.288764238357544,\n",
       " 2.288254737854004,\n",
       " 2.2878520488739014,\n",
       " 2.287599563598633,\n",
       " 2.2871813774108887,\n",
       " 2.2865397930145264,\n",
       " 2.2865748405456543,\n",
       " 2.28586483001709,\n",
       " 2.2848658561706543,\n",
       " 2.284860849380493,\n",
       " 2.2843000888824463,\n",
       " 2.2824673652648926,\n",
       " 2.282015323638916,\n",
       " 2.2814595699310303,\n",
       " 2.281528949737549,\n",
       " 2.281425952911377,\n",
       " 2.2815964221954346,\n",
       " 2.281242847442627,\n",
       " 2.2806122303009033,\n",
       " 2.2800166606903076,\n",
       " 2.2788984775543213,\n",
       " 2.2783215045928955,\n",
       " 2.2780818939208984,\n",
       " 2.2772812843322754,\n",
       " 2.276954412460327,\n",
       " 2.2768428325653076,\n",
       " 2.276203155517578,\n",
       " 2.2757365703582764,\n",
       " 2.2753384113311768,\n",
       " 2.2753021717071533,\n",
       " 2.274707317352295,\n",
       " 2.2740464210510254,\n",
       " 2.2738916873931885,\n",
       " 2.273895025253296,\n",
       " 2.2728939056396484,\n",
       " 2.2720258235931396,\n",
       " 2.271761655807495,\n",
       " 2.2706539630889893,\n",
       " 2.270540475845337,\n",
       " 2.2701494693756104,\n",
       " 2.2700159549713135,\n",
       " 2.2695605754852295,\n",
       " 2.269274950027466,\n",
       " 2.2684669494628906,\n",
       " 2.2675442695617676,\n",
       " 2.266540765762329,\n",
       " 2.265895128250122,\n",
       " 2.265615701675415,\n",
       " 2.2655580043792725,\n",
       " 2.264721393585205,\n",
       " 2.2638986110687256,\n",
       " 2.263540029525757,\n",
       " 2.2628185749053955,\n",
       " 2.2622761726379395,\n",
       " 2.2622296810150146,\n",
       " 2.2614190578460693,\n",
       " 2.2609806060791016,\n",
       " 2.260864496231079,\n",
       " 2.260265588760376,\n",
       " 2.2593371868133545,\n",
       " 2.258509874343872,\n",
       " 2.257826805114746,\n",
       " 2.2576181888580322,\n",
       " 2.2569544315338135,\n",
       " 2.256547689437866,\n",
       " 2.2560718059539795,\n",
       " 2.2555031776428223,\n",
       " 2.2549777030944824,\n",
       " 2.25484561920166,\n",
       " 2.254535675048828,\n",
       " 2.2542307376861572,\n",
       " 2.2534666061401367,\n",
       " 2.2525761127471924,\n",
       " 2.2519772052764893,\n",
       " 2.25102162361145,\n",
       " 2.2503750324249268,\n",
       " 2.2500109672546387,\n",
       " 2.2496910095214844,\n",
       " 2.249450206756592,\n",
       " 2.2484917640686035,\n",
       " 2.247770309448242,\n",
       " 2.2471601963043213,\n",
       " 2.2465834617614746,\n",
       " 2.2458231449127197,\n",
       " 2.2449159622192383,\n",
       " 2.2443625926971436,\n",
       " 2.244065523147583,\n",
       " 2.2434017658233643,\n",
       " 2.242900848388672,\n",
       " 2.242298126220703,\n",
       " 2.24202299118042,\n",
       " 2.2408342361450195,\n",
       " 2.2405755519866943,\n",
       " 2.240074396133423,\n",
       " 2.239595651626587,\n",
       " 2.238922119140625,\n",
       " 2.2384796142578125,\n",
       " 2.237790822982788,\n",
       " 2.236724376678467,\n",
       " 2.2360706329345703,\n",
       " 2.2353053092956543,\n",
       " 2.2346065044403076,\n",
       " 2.2339894771575928,\n",
       " 2.2334723472595215,\n",
       " 2.233051300048828,\n",
       " 2.2325122356414795,\n",
       " 2.231773853302002,\n",
       " 2.231017827987671,\n",
       " 2.2305667400360107,\n",
       " 2.2299444675445557,\n",
       " 2.229029893875122,\n",
       " 2.228809118270874,\n",
       " 2.228071689605713,\n",
       " 2.227322816848755,\n",
       " 2.2269885540008545,\n",
       " 2.2267537117004395,\n",
       " 2.2262914180755615,\n",
       " 2.225564956665039,\n",
       " 2.224865436553955,\n",
       " 2.2246055603027344,\n",
       " 2.2242071628570557,\n",
       " 2.223752498626709,\n",
       " 2.2231533527374268,\n",
       " 2.222459554672241,\n",
       " 2.221991777420044,\n",
       " 2.221137046813965,\n",
       " 2.220324754714966,\n",
       " 2.219602346420288,\n",
       " 2.218808650970459,\n",
       " 2.217996597290039,\n",
       " 2.217439651489258,\n",
       " 2.2169110774993896,\n",
       " 2.216042995452881,\n",
       " 2.21553373336792,\n",
       " 2.2150678634643555,\n",
       " 2.2141828536987305,\n",
       " 2.213514566421509,\n",
       " 2.2129342555999756,\n",
       " 2.2123570442199707,\n",
       " 2.211517810821533,\n",
       " 2.2109498977661133,\n",
       " 2.2100141048431396,\n",
       " 2.208794593811035,\n",
       " 2.208239793777466,\n",
       " 2.2071340084075928,\n",
       " 2.2065799236297607,\n",
       " 2.2059507369995117,\n",
       " 2.2051665782928467,\n",
       " 2.204463243484497,\n",
       " 2.203967332839966,\n",
       " 2.2031891345977783,\n",
       " 2.20192813873291,\n",
       " 2.201357841491699,\n",
       " 2.2007532119750977,\n",
       " 2.2001171112060547,\n",
       " 2.1991937160491943,\n",
       " 2.198413133621216,\n",
       " 2.198000431060791,\n",
       " 2.197335958480835,\n",
       " 2.196650743484497,\n",
       " 2.195574998855591,\n",
       " 2.195172071456909,\n",
       " 2.1944947242736816,\n",
       " 2.1936299800872803,\n",
       " 2.193131446838379,\n",
       " 2.192760705947876,\n",
       " 2.192169427871704,\n",
       " 2.19148325920105,\n",
       " 2.1907315254211426,\n",
       " 2.189685344696045,\n",
       " 2.1886065006256104,\n",
       " 2.1879312992095947,\n",
       " 2.1869471073150635,\n",
       " 2.185978651046753,\n",
       " 2.1850574016571045,\n",
       " 2.184403657913208,\n",
       " 2.1840858459472656,\n",
       " 2.18306827545166,\n",
       " 2.1822962760925293,\n",
       " 2.1816225051879883,\n",
       " 2.180806875228882,\n",
       " 2.1796958446502686,\n",
       " 2.178962469100952,\n",
       " 2.178114891052246,\n",
       " 2.1773080825805664,\n",
       " 2.1765620708465576,\n",
       " 2.175706386566162,\n",
       " 2.1751182079315186,\n",
       " 2.1745855808258057,\n",
       " 2.1735970973968506,\n",
       " 2.1725997924804688,\n",
       " 2.171827793121338,\n",
       " 2.1706337928771973,\n",
       " 2.1704282760620117,\n",
       " 2.1699860095977783,\n",
       " 2.1690869331359863,\n",
       " 2.168138027191162,\n",
       " 2.1670875549316406,\n",
       " 2.166213274002075,\n",
       " 2.1653192043304443,\n",
       " 2.1644070148468018,\n",
       " 2.1633548736572266,\n",
       " 2.1626391410827637,\n",
       " 2.161978006362915,\n",
       " 2.1605799198150635,\n",
       " 2.1596367359161377,\n",
       " 2.1589677333831787,\n",
       " 2.1582255363464355,\n",
       " 2.1577742099761963,\n",
       " 2.1566364765167236,\n",
       " 2.155851364135742,\n",
       " 2.1548988819122314,\n",
       " 2.1536710262298584,\n",
       " 2.15281081199646,\n",
       " 2.152125120162964,\n",
       " 2.151174306869507,\n",
       " 2.150181531906128,\n",
       " 2.149214506149292,\n",
       " 2.1484107971191406,\n",
       " 2.1473426818847656,\n",
       " 2.1466574668884277,\n",
       " 2.1456055641174316,\n",
       " 2.144826889038086,\n",
       " 2.143876075744629,\n",
       " 2.1430251598358154,\n",
       " 2.141780138015747,\n",
       " 2.1409261226654053,\n",
       " 2.1401255130767822,\n",
       " 2.1386735439300537,\n",
       " 2.1377716064453125,\n",
       " 2.136786699295044,\n",
       " 2.135801315307617,\n",
       " 2.134608745574951,\n",
       " 2.133544445037842,\n",
       " 2.1325936317443848,\n",
       " 2.131838798522949,\n",
       " 2.1309313774108887,\n",
       " 2.129868268966675,\n",
       " 2.1285548210144043,\n",
       " 2.1274430751800537,\n",
       " 2.1265816688537598,\n",
       " 2.1255037784576416,\n",
       " 2.1244983673095703,\n",
       " 2.1233768463134766,\n",
       " 2.1224451065063477,\n",
       " 2.121577501296997,\n",
       " 2.1203596591949463,\n",
       " 2.1192781925201416,\n",
       " 2.118279218673706,\n",
       " 2.117335319519043,\n",
       " 2.115980625152588,\n",
       " 2.1153018474578857,\n",
       " 2.1143133640289307,\n",
       " 2.112701892852783,\n",
       " 2.111325740814209,\n",
       " 2.1102447509765625,\n",
       " 2.108907461166382,\n",
       " 2.108029365539551,\n",
       " 2.1066598892211914,\n",
       " 2.105541706085205,\n",
       " 2.1043918132781982,\n",
       " 2.1033458709716797,\n",
       " 2.1022825241088867,\n",
       " 2.1008641719818115,\n",
       " 2.099574327468872,\n",
       " 2.0984272956848145,\n",
       " 2.096957206726074,\n",
       " 2.095890760421753,\n",
       " 2.094426155090332,\n",
       " 2.0930850505828857,\n",
       " 2.0918819904327393,\n",
       " 2.09090256690979,\n",
       " 2.0899553298950195,\n",
       " 2.089043140411377,\n",
       " 2.087871551513672,\n",
       " 2.0867786407470703,\n",
       " 2.085627317428589,\n",
       " 2.084536075592041,\n",
       " 2.0834720134735107,\n",
       " 2.082423448562622,\n",
       " 2.0813028812408447,\n",
       " 2.0801339149475098,\n",
       " 2.079012155532837,\n",
       " 2.0780560970306396,\n",
       " 2.0767269134521484,\n",
       " 2.075080156326294,\n",
       " 2.0732171535491943,\n",
       " 2.0717546939849854,\n",
       " 2.0704739093780518,\n",
       " 2.069404363632202,\n",
       " 2.067828416824341,\n",
       " 2.066540241241455,\n",
       " 2.0655455589294434,\n",
       " 2.064485549926758,\n",
       " 2.0631330013275146,\n",
       " 2.061741352081299,\n",
       " 2.0605361461639404,\n",
       " 2.059727191925049,\n",
       " 2.0583484172821045,\n",
       " 2.0572292804718018,\n",
       " 2.0560646057128906,\n",
       " 2.054788827896118,\n",
       " 2.05325984954834,\n",
       " 2.0522267818450928,\n",
       " 2.051021099090576,\n",
       " 2.0497708320617676,\n",
       " 2.048779010772705,\n",
       " 2.0475547313690186,\n",
       " 2.0461764335632324,\n",
       " 2.044771671295166,\n",
       " 2.0434560775756836,\n",
       " 2.04225754737854,\n",
       " 2.0410799980163574,\n",
       " 2.039794445037842,\n",
       " 2.03857159614563,\n",
       " 2.0370335578918457,\n",
       " 2.035388469696045,\n",
       " 2.0339982509613037,\n",
       " 2.032881498336792,\n",
       " 2.031729221343994,\n",
       " 2.030322790145874,\n",
       " 2.0285463333129883,\n",
       " 2.0273921489715576,\n",
       " 2.025900363922119,\n",
       " 2.0245423316955566,\n",
       " 2.0233304500579834,\n",
       " 2.0218729972839355,\n",
       " 2.020768880844116,\n",
       " 2.019469976425171,\n",
       " 2.0180792808532715,\n",
       " 2.016573190689087,\n",
       " 2.014944553375244,\n",
       " 2.013664960861206,\n",
       " 2.012190580368042,\n",
       " 2.0102219581604004,\n",
       " 2.008683443069458,\n",
       " 2.006904125213623,\n",
       " 2.0053515434265137,\n",
       " 2.00358247756958,\n",
       " 2.002025842666626,\n",
       " 2.000654697418213,\n",
       " 1.9989625215530396,\n",
       " 1.9976692199707031,\n",
       " 1.9962081909179688,\n",
       " 1.994847059249878,\n",
       " 1.993282675743103,\n",
       " 1.99184250831604,\n",
       " 1.9906474351882935,\n",
       " 1.9893782138824463,\n",
       " 1.987811803817749,\n",
       " 1.9861055612564087,\n",
       " 1.9840707778930664,\n",
       " 1.9827759265899658,\n",
       " 1.9814549684524536,\n",
       " 1.9801396131515503,\n",
       " 1.9789499044418335,\n",
       " 1.9773825407028198,\n",
       " 1.9757812023162842,\n",
       " 1.9741885662078857,\n",
       " 1.9730247259140015,\n",
       " 1.9712797403335571,\n",
       " 1.9699653387069702,\n",
       " 1.9687350988388062,\n",
       " 1.9670137166976929,\n",
       " 1.9657593965530396,\n",
       " 1.9643917083740234,\n",
       " 1.9628926515579224,\n",
       " 1.9611886739730835,\n",
       " 1.9594985246658325,\n",
       " 1.9582208395004272,\n",
       " 1.9568463563919067,\n",
       " 1.9554587602615356,\n",
       " 1.9540435075759888,\n",
       " 1.9523868560791016,\n",
       " 1.9508490562438965,\n",
       " 1.9489259719848633,\n",
       " 1.946820855140686,\n",
       " 1.9454550743103027,\n",
       " 1.943631649017334,\n",
       " 1.9419430494308472,\n",
       " 1.9403226375579834,\n",
       " 1.9388362169265747,\n",
       " 1.9372272491455078,\n",
       " 1.9355319738388062,\n",
       " 1.9339628219604492,\n",
       " 1.932276964187622,\n",
       " 1.9307529926300049,\n",
       " 1.9289888143539429,\n",
       " 1.9272208213806152,\n",
       " 1.9253097772598267,\n",
       " 1.9236090183258057,\n",
       " 1.9218796491622925,\n",
       " 1.9202308654785156,\n",
       " 1.9187170267105103,\n",
       " 1.9171158075332642,\n",
       " 1.9151889085769653,\n",
       " 1.913306713104248,\n",
       " 1.911380648612976,\n",
       " 1.9100288152694702,\n",
       " 1.9080462455749512,\n",
       " 1.9062012434005737,\n",
       " 1.9045130014419556,\n",
       " 1.9031245708465576,\n",
       " 1.9013807773590088,\n",
       " 1.8996657133102417,\n",
       " 1.8979185819625854,\n",
       " 1.8960574865341187,\n",
       " 1.8946137428283691,\n",
       " 1.8927314281463623,\n",
       " 1.8907099962234497,\n",
       " 1.8887594938278198,\n",
       " 1.8870701789855957,\n",
       " 1.8853760957717896,\n",
       " 1.8833928108215332,\n",
       " 1.8817641735076904,\n",
       " 1.8797967433929443,\n",
       " 1.8783212900161743,\n",
       " 1.8764349222183228,\n",
       " 1.8741016387939453,\n",
       " 1.8724710941314697,\n",
       " 1.8708760738372803,\n",
       " 1.869509220123291,\n",
       " 1.8679118156433105,\n",
       " 1.8661643266677856,\n",
       " 1.864367961883545,\n",
       " 1.8622092008590698,\n",
       " 1.8606520891189575,\n",
       " 1.8587090969085693,\n",
       " 1.8567233085632324,\n",
       " 1.854504942893982,\n",
       " 1.8526133298873901,\n",
       " 1.8507006168365479,\n",
       " 1.8485759496688843,\n",
       " 1.8467215299606323,\n",
       " 1.845305323600769,\n",
       " 1.8434885740280151,\n",
       " 1.8414978981018066,\n",
       " 1.8398947715759277,\n",
       " 1.838425874710083,\n",
       " 1.8366769552230835,\n",
       " 1.8351380825042725,\n",
       " 1.833776593208313,\n",
       " 1.8318817615509033,\n",
       " 1.8304203748703003,\n",
       " 1.8292349576950073,\n",
       " 1.827369213104248,\n",
       " 1.825815200805664,\n",
       " 1.8242723941802979,\n",
       " 1.8224409818649292,\n",
       " 1.8208813667297363,\n",
       " 1.8190898895263672,\n",
       " 1.8171439170837402,\n",
       " 1.8152426481246948,\n",
       " 1.8130578994750977,\n",
       " 1.8113999366760254,\n",
       " 1.8097100257873535,\n",
       " 1.807889699935913,\n",
       " 1.806209921836853,\n",
       " 1.8041162490844727,\n",
       " 1.8024564981460571,\n",
       " 1.8008091449737549,\n",
       " 1.7989965677261353,\n",
       " 1.7973684072494507,\n",
       " 1.795624017715454,\n",
       " 1.794055461883545,\n",
       " 1.7920479774475098,\n",
       " 1.7904176712036133,\n",
       " 1.7884434461593628,\n",
       " 1.7864640951156616,\n",
       " 1.7847843170166016,\n",
       " 1.7827664613723755,\n",
       " 1.7814409732818604,\n",
       " 1.7795028686523438,\n",
       " 1.7777289152145386,\n",
       " 1.7758173942565918,\n",
       " 1.7741084098815918,\n",
       " 1.772339105606079,\n",
       " 1.7708755731582642,\n",
       " 1.7694720029830933,\n",
       " 1.7676126956939697,\n",
       " 1.7658182382583618,\n",
       " 1.7640211582183838,\n",
       " 1.7622612714767456,\n",
       " 1.760450839996338,\n",
       " 1.7588530778884888,\n",
       " 1.7571923732757568,\n",
       " 1.755837321281433,\n",
       " 1.7541059255599976,\n",
       " 1.7524491548538208,\n",
       " 1.7507041692733765,\n",
       " 1.7489477396011353,\n",
       " 1.7472482919692993,\n",
       " 1.7458409070968628,\n",
       " 1.743963360786438,\n",
       " 1.7425241470336914,\n",
       " 1.7406070232391357,\n",
       " 1.7387640476226807,\n",
       " 1.737067699432373,\n",
       " 1.7355607748031616,\n",
       " 1.7335968017578125,\n",
       " 1.7319416999816895,\n",
       " 1.7302570343017578,\n",
       " 1.728583812713623,\n",
       " 1.7273924350738525,\n",
       " 1.7256478071212769,\n",
       " 1.7238911390304565,\n",
       " 1.7218098640441895,\n",
       " 1.7199324369430542,\n",
       " 1.717848777770996,\n",
       " 1.7160305976867676,\n",
       " 1.714362621307373,\n",
       " 1.7127982378005981,\n",
       " 1.710925817489624,\n",
       " 1.70912766456604,\n",
       " 1.7075653076171875,\n",
       " 1.7054704427719116,\n",
       " 1.7035773992538452,\n",
       " 1.7019152641296387,\n",
       " 1.700155258178711,\n",
       " 1.6984831094741821,\n",
       " 1.6967084407806396,\n",
       " 1.6952674388885498,\n",
       " 1.6935536861419678,\n",
       " 1.6916753053665161,\n",
       " 1.6900044679641724,\n",
       " 1.6881442070007324,\n",
       " 1.686561107635498,\n",
       " 1.6849453449249268,\n",
       " 1.6832104921340942,\n",
       " 1.6819500923156738,\n",
       " 1.6804121732711792,\n",
       " 1.6790549755096436,\n",
       " 1.6776782274246216,\n",
       " 1.676045536994934,\n",
       " 1.6740437746047974,\n",
       " 1.6725269556045532,\n",
       " 1.6709225177764893,\n",
       " 1.66926109790802,\n",
       " 1.6672428846359253,\n",
       " 1.6655138731002808,\n",
       " 1.663639783859253,\n",
       " 1.6615890264511108,\n",
       " 1.6599379777908325,\n",
       " 1.657918930053711,\n",
       " 1.6562741994857788,\n",
       " 1.6545569896697998,\n",
       " 1.6529337167739868,\n",
       " 1.6513062715530396,\n",
       " 1.6492242813110352,\n",
       " 1.647565484046936,\n",
       " 1.6457730531692505,\n",
       " 1.6444162130355835,\n",
       " 1.6427075862884521,\n",
       " 1.6411250829696655,\n",
       " 1.639346957206726,\n",
       " 1.6376546621322632,\n",
       " 1.636160969734192,\n",
       " 1.6344027519226074,\n",
       " 1.6328202486038208,\n",
       " 1.6313931941986084,\n",
       " 1.6296770572662354,\n",
       " 1.6278576850891113,\n",
       " 1.62623929977417,\n",
       " 1.6242903470993042,\n",
       " 1.6230945587158203,\n",
       " 1.6216113567352295,\n",
       " 1.6197755336761475,\n",
       " 1.618114709854126,\n",
       " 1.6165186166763306,\n",
       " 1.6146550178527832,\n",
       " 1.6131701469421387,\n",
       " 1.611601710319519,\n",
       " 1.6097519397735596,\n",
       " 1.6083104610443115,\n",
       " 1.606573224067688,\n",
       " 1.6050047874450684,\n",
       " 1.6030151844024658,\n",
       " 1.6016769409179688,\n",
       " 1.600184440612793,\n",
       " 1.5985946655273438,\n",
       " 1.596789836883545,\n",
       " 1.595211148262024,\n",
       " 1.5935403108596802,\n",
       " 1.5918099880218506,\n",
       " 1.590482234954834,\n",
       " 1.5886448621749878,\n",
       " 1.587109923362732,\n",
       " 1.5854756832122803,\n",
       " 1.5838744640350342,\n",
       " 1.5822482109069824,\n",
       " 1.5806688070297241,\n",
       " 1.5788371562957764,\n",
       " 1.5770925283432007,\n",
       " 1.5753475427627563,\n",
       " 1.5739235877990723,\n",
       " 1.5722754001617432,\n",
       " 1.570802927017212,\n",
       " 1.5690407752990723,\n",
       " 1.5676331520080566,\n",
       " 1.565921425819397,\n",
       " 1.5645723342895508,\n",
       " 1.5630398988723755,\n",
       " 1.5619641542434692,\n",
       " 1.560534119606018,\n",
       " 1.5589590072631836,\n",
       " 1.5579599142074585,\n",
       " 1.5563586950302124,\n",
       " 1.554785966873169,\n",
       " 1.5533357858657837,\n",
       " 1.5518871545791626,\n",
       " 1.5503557920455933,\n",
       " 1.5488691329956055,\n",
       " 1.5475833415985107,\n",
       " 1.5463911294937134,\n",
       " 1.5448466539382935,\n",
       " 1.543262004852295,\n",
       " 1.5418397188186646,\n",
       " 1.5404586791992188,\n",
       " 1.5388373136520386,\n",
       " 1.5373101234436035,\n",
       " 1.5356738567352295,\n",
       " 1.5338630676269531,\n",
       " 1.5323376655578613,\n",
       " 1.5306302309036255,\n",
       " 1.5292174816131592,\n",
       " 1.5277289152145386,\n",
       " 1.5263186693191528,\n",
       " 1.524815320968628,\n",
       " 1.523187279701233,\n",
       " 1.5217676162719727,\n",
       " 1.5201992988586426,\n",
       " 1.518763780593872,\n",
       " 1.5171161890029907,\n",
       " 1.515738844871521,\n",
       " 1.5142313241958618,\n",
       " 1.5126843452453613,\n",
       " 1.5111125707626343,\n",
       " 1.5095547437667847,\n",
       " 1.5084302425384521,\n",
       " 1.5067024230957031,\n",
       " 1.5050305128097534,\n",
       " 1.5035916566848755,\n",
       " 1.502130150794983,\n",
       " 1.5006108283996582,\n",
       " 1.4992674589157104,\n",
       " 1.497942566871643,\n",
       " 1.4967304468154907,\n",
       " 1.4954874515533447,\n",
       " 1.4940515756607056,\n",
       " 1.4924389123916626,\n",
       " 1.4911786317825317,\n",
       " 1.4898768663406372,\n",
       " 1.4885505437850952,\n",
       " 1.4873250722885132,\n",
       " 1.485733151435852,\n",
       " 1.4842041730880737,\n",
       " 1.4826096296310425,\n",
       " 1.4809823036193848,\n",
       " 1.4795372486114502,\n",
       " 1.4779977798461914,\n",
       " 1.4767348766326904,\n",
       " 1.4754818677902222,\n",
       " 1.474079966545105,\n",
       " 1.4725019931793213,\n",
       " 1.4710438251495361,\n",
       " 1.4695942401885986,\n",
       " 1.4680129289627075,\n",
       " 1.4664945602416992,\n",
       " 1.4648809432983398,\n",
       " 1.463760495185852,\n",
       " 1.462405800819397,\n",
       " 1.4610626697540283,\n",
       " 1.4594347476959229,\n",
       " 1.4582087993621826,\n",
       " 1.4567025899887085,\n",
       " 1.4554247856140137,\n",
       " 1.4541302919387817,\n",
       " 1.453001618385315,\n",
       " 1.451512336730957,\n",
       " 1.4500069618225098,\n",
       " 1.4486981630325317,\n",
       " 1.4476077556610107,\n",
       " 1.4464409351348877,\n",
       " 1.4449074268341064,\n",
       " 1.4434202909469604,\n",
       " 1.4418162107467651,\n",
       " 1.4402765035629272,\n",
       " 1.4388763904571533,\n",
       " 1.4375956058502197,\n",
       " 1.4364230632781982,\n",
       " 1.4348835945129395,\n",
       " 1.4336004257202148,\n",
       " 1.4322148561477661,\n",
       " 1.4307135343551636,\n",
       " 1.4294198751449585,\n",
       " 1.4282777309417725,\n",
       " 1.4272571802139282,\n",
       " 1.4261658191680908,\n",
       " 1.4251309633255005,\n",
       " 1.424052119255066,\n",
       " 1.4230525493621826,\n",
       " 1.421783447265625,\n",
       " 1.420481562614441,\n",
       " 1.4190657138824463,\n",
       " 1.4179977178573608,\n",
       " 1.41649329662323,\n",
       " 1.4154162406921387,\n",
       " 1.4143081903457642,\n",
       " 1.4130197763442993,\n",
       " 1.4115746021270752,\n",
       " 1.410292148590088,\n",
       " 1.4092295169830322,\n",
       " 1.4082870483398438,\n",
       " 1.407232642173767,\n",
       " 1.406029224395752,\n",
       " 1.4048866033554077,\n",
       " 1.4038549661636353,\n",
       " 1.4026857614517212,\n",
       " 1.4014806747436523,\n",
       " 1.4002490043640137,\n",
       " 1.3987926244735718,\n",
       " 1.3975492715835571,\n",
       " 1.3963967561721802,\n",
       " 1.3953709602355957,\n",
       " 1.3940693140029907,\n",
       " 1.3927741050720215,\n",
       " 1.3914850950241089,\n",
       " 1.3902091979980469,\n",
       " 1.3893064260482788,\n",
       " 1.3882945775985718,\n",
       " 1.3868781328201294,\n",
       " 1.385693073272705,\n",
       " 1.384383201599121,\n",
       " 1.3831298351287842,\n",
       " 1.3819586038589478,\n",
       " 1.3809300661087036,\n",
       " 1.3798397779464722,\n",
       " 1.378914713859558,\n",
       " 1.3777403831481934,\n",
       " 1.3765966892242432,\n",
       " 1.3751128911972046,\n",
       " 1.3738856315612793,\n",
       " 1.372533917427063,\n",
       " 1.371567726135254,\n",
       " 1.3706285953521729,\n",
       " 1.369336724281311,\n",
       " 1.3680238723754883,\n",
       " 1.3667045831680298,\n",
       " 1.3654673099517822,\n",
       " 1.3642480373382568,\n",
       " 1.3633091449737549,\n",
       " 1.3623487949371338,\n",
       " 1.3617069721221924,\n",
       " 1.360460638999939,\n",
       " 1.3592113256454468,\n",
       " 1.3580775260925293,\n",
       " 1.3568981885910034,\n",
       " 1.355646014213562,\n",
       " 1.3544461727142334,\n",
       " 1.353062391281128,\n",
       " 1.351925015449524,\n",
       " 1.3507968187332153,\n",
       " 1.3494549989700317,\n",
       " 1.348376989364624,\n",
       " 1.3471511602401733,\n",
       " 1.3460888862609863,\n",
       " 1.345061182975769,\n",
       " 1.3438796997070312,\n",
       " 1.3426824808120728,\n",
       " 1.34160315990448,\n",
       " 1.340564489364624,\n",
       " 1.3394818305969238,\n",
       " 1.3383662700653076,\n",
       " 1.3372085094451904,\n",
       " 1.3361549377441406,\n",
       " 1.3351819515228271,\n",
       " 1.334080696105957,\n",
       " 1.3328551054000854,\n",
       " 1.3317643404006958,\n",
       " 1.330540418624878,\n",
       " 1.3292044401168823,\n",
       " 1.3281770944595337,\n",
       " 1.327133059501648,\n",
       " 1.3264106512069702,\n",
       " 1.3256723880767822,\n",
       " 1.3244727849960327,\n",
       " 1.3233331441879272,\n",
       " 1.3221831321716309,\n",
       " 1.3213659524917603,\n",
       " 1.3203219175338745,\n",
       " 1.3193060159683228,\n",
       " 1.3181145191192627,\n",
       " 1.316996455192566,\n",
       " 1.315649151802063,\n",
       " 1.3145695924758911,\n",
       " 1.3136718273162842,\n",
       " 1.3124338388442993,\n",
       " 1.3116583824157715,\n",
       " 1.3106892108917236,\n",
       " 1.3095979690551758,\n",
       " 1.3086379766464233,\n",
       " 1.307741641998291,\n",
       " 1.3066835403442383,\n",
       " 1.3056399822235107,\n",
       " 1.3049756288528442,\n",
       " 1.3037846088409424,\n",
       " 1.3028017282485962,\n",
       " 1.3015005588531494,\n",
       " 1.3003257513046265,\n",
       " 1.2992292642593384,\n",
       " 1.2981834411621094,\n",
       " 1.2972021102905273,\n",
       " 1.2961543798446655,\n",
       " 1.2949551343917847,\n",
       " 1.2939050197601318,\n",
       " 1.293357014656067,\n",
       " 1.292338252067566,\n",
       " 1.2913100719451904,\n",
       " 1.2902230024337769,\n",
       " 1.2893006801605225,\n",
       " 1.2881988286972046,\n",
       " 1.2875605821609497,\n",
       " 1.2867798805236816,\n",
       " 1.2860029935836792,\n",
       " 1.2852897644042969,\n",
       " 1.2842265367507935,\n",
       " 1.2830677032470703,\n",
       " 1.2823740243911743,\n",
       " 1.2812527418136597,\n",
       " 1.2801234722137451,\n",
       " 1.2794520854949951,\n",
       " 1.2785043716430664,\n",
       " 1.277443289756775,\n",
       " 1.2763563394546509,\n",
       " 1.2753560543060303,\n",
       " 1.2742054462432861,\n",
       " 1.2734867334365845,\n",
       " 1.2724679708480835,\n",
       " 1.2715612649917603,\n",
       " 1.270695686340332,\n",
       " 1.2698674201965332,\n",
       " 1.2687143087387085,\n",
       " 1.2677278518676758,\n",
       " 1.266940951347351,\n",
       " 1.2663332223892212,\n",
       " 1.2653974294662476,\n",
       " 1.2643985748291016,\n",
       " 1.2635931968688965,\n",
       " 1.2625151872634888,\n",
       " 1.2614672183990479,\n",
       " 1.260697841644287,\n",
       " 1.2600550651550293,\n",
       " 1.259093999862671,\n",
       " 1.2580972909927368,\n",
       " 1.2574230432510376,\n",
       " 1.25682532787323,\n",
       " 1.255718469619751,\n",
       " 1.2548085451126099,\n",
       " 1.2536386251449585,\n",
       " 1.252591848373413,\n",
       " 1.2516868114471436,\n",
       " 1.250771403312683,\n",
       " 1.2498888969421387,\n",
       " 1.2491000890731812,\n",
       " 1.248260498046875,\n",
       " 1.2473671436309814,\n",
       " 1.2465295791625977,\n",
       " 1.2457081079483032,\n",
       " 1.2447936534881592,\n",
       " 1.2440135478973389,\n",
       " 1.243025779724121,\n",
       " 1.2421618700027466,\n",
       " 1.2412426471710205,\n",
       " 1.2404367923736572,\n",
       " 1.2394614219665527,\n",
       " 1.2384685277938843,\n",
       " 1.2374541759490967,\n",
       " 1.2364259958267212,\n",
       " 1.235444188117981,\n",
       " 1.2344379425048828,\n",
       " 1.2336126565933228,\n",
       " 1.2325315475463867,\n",
       " 1.2315311431884766,\n",
       " 1.2307677268981934,\n",
       " 1.2299706935882568,\n",
       " 1.2292139530181885,\n",
       " 1.2282177209854126,\n",
       " 1.227339744567871,\n",
       " 1.2265104055404663,\n",
       " 1.225548505783081,\n",
       " 1.2244873046875,\n",
       " 1.2233532667160034,\n",
       " 1.2222285270690918,\n",
       " 1.2211822271347046,\n",
       " 1.220190405845642,\n",
       " 1.219252109527588,\n",
       " 1.218413233757019,\n",
       " 1.2177245616912842,\n",
       " 1.2169150114059448,\n",
       " 1.2161556482315063,\n",
       " 1.2152409553527832,\n",
       " 1.2142475843429565,\n",
       " 1.2133841514587402,\n",
       " 1.2124273777008057,\n",
       " 1.2115191221237183,\n",
       " 1.2106183767318726,\n",
       " 1.209895133972168,\n",
       " 1.208748459815979,\n",
       " 1.2077031135559082,\n",
       " 1.2066551446914673,\n",
       " 1.2058771848678589,\n",
       " 1.205039620399475,\n",
       " 1.2039833068847656,\n",
       " 1.2033002376556396,\n",
       " 1.2024205923080444,\n",
       " 1.2014578580856323,\n",
       " 1.2003589868545532,\n",
       " 1.1993235349655151,\n",
       " 1.1983811855316162,\n",
       " 1.197546124458313,\n",
       " 1.1970624923706055,\n",
       " 1.1961376667022705,\n",
       " 1.1952285766601562,\n",
       " 1.1942106485366821,\n",
       " 1.1941640377044678,\n",
       " ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expon_lr.losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c8f63b",
   "metadata": {},
   "source": [
    "그래프를 보고 학습률를 튜닝해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0768d4f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2333 - accuracy: 0.9267 - val_loss: 0.1018 - val_accuracy: 0.9688\n",
      "Epoch 2/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0941 - accuracy: 0.9708 - val_loss: 0.0995 - val_accuracy: 0.9730\n",
      "Epoch 3/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0643 - accuracy: 0.9802 - val_loss: 0.0771 - val_accuracy: 0.9768\n",
      "Epoch 4/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0469 - accuracy: 0.9849 - val_loss: 0.0730 - val_accuracy: 0.9810\n",
      "Epoch 5/100\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.0345 - accuracy: 0.9885 - val_loss: 0.0880 - val_accuracy: 0.9778\n",
      "Epoch 6/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0288 - accuracy: 0.9911 - val_loss: 0.0680 - val_accuracy: 0.9818\n",
      "Epoch 7/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0210 - accuracy: 0.9928 - val_loss: 0.0726 - val_accuracy: 0.9798\n",
      "Epoch 8/100\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.0181 - accuracy: 0.9938 - val_loss: 0.0775 - val_accuracy: 0.9838\n",
      "Epoch 9/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0135 - accuracy: 0.9957 - val_loss: 0.0890 - val_accuracy: 0.9824\n",
      "Epoch 10/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0137 - accuracy: 0.9954 - val_loss: 0.0895 - val_accuracy: 0.9800\n",
      "Epoch 11/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0116 - accuracy: 0.9960 - val_loss: 0.0879 - val_accuracy: 0.9820\n",
      "Epoch 12/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0111 - accuracy: 0.9964 - val_loss: 0.1035 - val_accuracy: 0.9788\n",
      "Epoch 13/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0104 - accuracy: 0.9968 - val_loss: 0.0874 - val_accuracy: 0.9820\n",
      "Epoch 14/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0087 - accuracy: 0.9972 - val_loss: 0.0919 - val_accuracy: 0.9838\n",
      "Epoch 15/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0105 - accuracy: 0.9966 - val_loss: 0.0928 - val_accuracy: 0.9800\n",
      "Epoch 16/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0040 - accuracy: 0.9987 - val_loss: 0.0811 - val_accuracy: 0.9826\n",
      "Epoch 17/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.0823 - val_accuracy: 0.9860\n",
      "Epoch 18/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 5.6057e-04 - accuracy: 0.9998 - val_loss: 0.0785 - val_accuracy: 0.9854\n",
      "Epoch 19/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 1.1999e-04 - accuracy: 1.0000 - val_loss: 0.0797 - val_accuracy: 0.9856\n",
      "Epoch 20/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 7.7460e-05 - accuracy: 1.0000 - val_loss: 0.0799 - val_accuracy: 0.9856\n",
      "Epoch 21/100\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 6.2582e-05 - accuracy: 1.0000 - val_loss: 0.0803 - val_accuracy: 0.9854\n",
      "Epoch 22/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 5.4441e-05 - accuracy: 1.0000 - val_loss: 0.0808 - val_accuracy: 0.9854\n",
      "Epoch 23/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 4.8599e-05 - accuracy: 1.0000 - val_loss: 0.0814 - val_accuracy: 0.9858\n",
      "Epoch 24/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 4.4041e-05 - accuracy: 1.0000 - val_loss: 0.0819 - val_accuracy: 0.9860\n",
      "Epoch 25/100\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 4.0573e-05 - accuracy: 1.0000 - val_loss: 0.0822 - val_accuracy: 0.9860\n",
      "Epoch 26/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 3.7558e-05 - accuracy: 1.0000 - val_loss: 0.0827 - val_accuracy: 0.9860\n"
     ]
    }
   ],
   "source": [
    "# clear_session\n",
    "# keras.backend.clear_session()\n",
    "# 현재 TF 그래프를 없애고, 새로운 TF 그래프를 만듭니다.\n",
    "#오래된 모델 혹은 층과의 혼란을 피할 때 유용합니다.\n",
    "keras.backend.clear_session()\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model_last = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dense(300, activation='relu'),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model_last.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=3e-1),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_mnist_model.h5\", save_best_only=True)\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "history = model_last.fit(X_train, y_train, epochs=100,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c95c95a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0690 - accuracy: 0.9814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06899699568748474, 0.9814000129699707]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model(\"my_mnist_model.h5\") # rollback to best model\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "284b36a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 13396), started 0:29:53 ago. (Use '!kill 13396' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-350402f3ea611a96\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-350402f3ea611a96\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=./my_mnist_logs --port=6006"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
