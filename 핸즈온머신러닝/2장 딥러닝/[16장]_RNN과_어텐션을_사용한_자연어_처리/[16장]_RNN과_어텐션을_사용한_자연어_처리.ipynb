{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "784a15c9",
      "metadata": {
        "id": "784a15c9"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donghui-0126/machine-learning/blob/main/%ED%95%B8%EC%A6%88%EC%98%A8%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/2%EC%9E%A5%20%EB%94%A5%EB%9F%AC%EB%8B%9D/%5B16%EC%9E%A5%5D_RNN%EA%B3%BC_%EC%96%B4%ED%85%90%EC%85%98%EC%9D%84_%EC%82%AC%EC%9A%A9%ED%95%9C_%EC%9E%90%EC%97%B0%EC%96%B4_%EC%B2%98%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "f3843735",
      "metadata": {
        "id": "f3843735"
      },
      "outputs": [],
      "source": [
        "# -*- conding:utf-8 -*-\n",
        "\n",
        "# 파이썬 ≥3.5 필수\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# 사이킷런 ≥0.20 필수\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "# 텐서플로 ≥2.0 필수\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"감지된 GPU가 없습니다. GPU가 없으면 CNN은 매우 느릴 수 있습니다.\")\n",
        "    \n",
        "\n",
        "# 공통 모듈 임포트\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# 노트북 실행 결과를 동일하게 유지하기 위해\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 깔끔한 그래프 출력을 위해\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "427271a7",
      "metadata": {
        "id": "427271a7"
      },
      "source": [
        "이번 장에서는 자연어를 읽고 쓰는 AI를 만드는 법을 알아볼 것이다. \n",
        "\n",
        "자연어 문제를 위해 많이 사용하는 방법은 순환 신경망이다. 따라서 RNN을 계속해서 살펴본다. 문장에서 다음글자를 예측하도록 훈련하는 **문자 단위 RNN(character RNN)** 부터 시작한다. 새로운 텍스트를 생성하고 그 과정에서 매우 긴 시퀀스를 가진 텐서플로 데이터셋을 만드는 방법을 알아보자. 먼저 **상태가 없는 RNN(stateless RNN)** 을 사용하고 다음에 **상태가 있는 RNN(stateful RNN)** 을 구축할 것이다. (상태가 없는 RNN은 각 반복에서 무작위하게 택한 텍스트의 일부분으로 학습하고, 나머지 텍스트에서 어떤 정보도 사용하지 않는다. 상태가 있는 RNN은 훈련 반복 사이에 은닉 상태를 유지하고 중지된 곳에서 이어서 상태를 반영한다. 그래서 더 긴패턴을 학습할 수 있다). 그 다음에는 감성분석을 수행하는 RNN을 구축하고 이번에는 문자가 아니라 단어의 시퀀스로 문장을 다룰것이다. 그리고 신경망 기계번역(NMT)을 수행할 수 있는 인코더-디코더 구조를 만들기 위해 RNN을 사용하는 방법을 알아보자. 도구로는 텐서플로 애드온 프로젝트에서 제공하는 seq2seq API를 사용한다.\n",
        "\n",
        "16.4절에서는 어텐션 매커니즘을 알아보자. 이름에서 알 수 있듯이 이는 각 타임 스텝에서 모델이 집중해야 할 입력 부분을 선택하도록 학습하는 신경망 구성요소이다. 먼저 어텐션을 사용하여 RNN 기반의 인코더-디코더 구조의 성능을 높이는 방법을 알아본 뒤, RNN을 모두 제거하고 어텐션만 사용해 매우좋은 성능을 내는 **트랜스포머** 라는 구조도 살펴보자.  마지막으로 NLP분야에서 가장 중요한 발전을 살펴보자. 트랜스포머를 시반으로 한 GPT-2와 BERT같은 매우 강력한 언어 모델도 포함한다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "348a1c7b",
      "metadata": {
        "id": "348a1c7b"
      },
      "source": [
        "# char-RNN을 사용해 셰익스피어 문체 생성하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "996d1d9f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "996d1d9f",
        "outputId": "0c2750ff-8ef3-40d2-9806-9b915ca626ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://homl.info/shakespeare\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "shakespeare_url = \"https://homl.info/shakespeare\"  # 단죽 url\n",
        "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
        "with open(filepath) as f:\n",
        "    shakespeare_text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "1ad9e061",
      "metadata": {
        "id": "1ad9e061"
      },
      "outputs": [],
      "source": [
        "# 모든 글자를 정수로 인코딩해준다. \n",
        "# 간단하게 Tokenizer 클래스를 사용한다.\n",
        "\n",
        "# char_level 을 True로 지정해서 단어 수준 인코딩 대신 글자 수준 인코딩을 만든다. 이 클래스는 기본적으로 텍스트를 소문자로 바꾼다.\n",
        "# 이제 문장을 글자 ID로 인코딩 하거나 반대로 디코딩 할 수 있다. \n",
        "# 이를 통해텍스트에 있는 고유 글자 개수와 전체 글자 개수를 알 수 있다.\n",
        "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(shakespeare_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "72c34059",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72c34059",
        "outputId": "55011faa-91b4-40eb-e13d-114669e4a177"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[20, 6, 9, 8, 3]]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "tokenizer.texts_to_sequences([\"First\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e24d084b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e24d084b",
        "outputId": "2fcb8009-a5f1-43e3-e592-7b4caf17ccfe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['f i r s t']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "tokenizer.sequences_to_texts([[20,6,9,8,3]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e7695e76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7695e76",
        "outputId": "bc133390-261e-4aa1-ba61-0653c13fa18f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# 고유 글자 개수\n",
        "max_id = len(tokenizer.word_index)\n",
        "max_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ba9b5ef9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba9b5ef9",
        "outputId": "2760000d-2692-4635-bd66-a18528fdf8e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1115394"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# 전체 글자 개수\n",
        "dataset_size = tokenizer.document_count \n",
        "dataset_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a6d7f28c",
      "metadata": {
        "id": "a6d7f28c"
      },
      "outputs": [],
      "source": [
        "# 전체 텍스트를 인코딩해서 각 글자를 ID로 나타내 보자.\n",
        "# 1~ 39인 값을 0~38로 바꾸기\n",
        "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "49bf0e39",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49bf0e39",
        "outputId": "652baa6f-f0b9-4bb3-e656-6f54f0e887c6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1115394,)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "encoded.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7f9a744",
      "metadata": {
        "id": "e7f9a744"
      },
      "source": [
        "## 순차 데이터셋을 나누는 방법\n",
        "훈련셋, 테스트셋, 검증셋을 중복되지 않도록 만드는 것은 중요하다.\n",
        "\n",
        "보통 시계열을 다룰 때는 시간에 따라 나눈다. 다른 차원으로 나눌 수 있지만, train set 과 test set 사이의 상관관계가 있다면 오차를 낙관적으로 측정해서 모델이 유용하지 않을 것이다. \n",
        "\n",
        "시계열 데이터가 시간에 따라서 반복되는 **변하지 않는 상태** 라면 시계열을 안정적으로 분석할 수 있다. 하지만 그렇지 않은 시계열도 있을 것이다. \n",
        "\n",
        "시계열이 안정적인지 확인할 방법은 시간에 따라 검증 세트에 대한 모델의 오차를 그리는 것이다. 모델이 검증 세트 마지막 보다 첫 부분에서 성능이 더 좋다면 이 시계열이 충분히 안정되지 않을 것일 수 있다. 이런 경우에는 **더 짧은 사건 간격으로** 모델을 훈련하는 게 좋다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "a4160b18",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4160b18",
        "outputId": "df0f3e63-0ec4-4a3a-949d-3e3958a3f7a5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1003854"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "# 셰익스피어 텍스트의 처음 90%를 trainset으로 사용하고 나머지는 validset과 testset로 사용한다. \n",
        "# 이 set에서 한 번에 한 글자씩 반환하는 tf.data.Dataset 객체를 만든다.\n",
        "train_size = dataset_size * 90 // 100\n",
        "dataset1 = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
        "train_size"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f04f7ee",
      "metadata": {
        "id": "6f04f7ee"
      },
      "source": [
        "## 순차 데이터를 윈도 여러개로 자르기\n",
        "\n",
        "https://teddylee777.github.io/tensorflow/dataset-batch-window  이해가 잘 안되서 찾아본 블로그이다. \n",
        "\n",
        "trainset은 백만개 이상의 글자로 이루어진 시퀀스 하나이다. 여기에 신경망을 직접 훈련시킬수는 없다. 이 RNN은 백만 개의 층이 있는 심층 신경망과 비슷하고 (매우 긴) 샘플 하나로 훈련하는 셈이 된다. 대신 데이터셋의 window() 메서드를 사용해 이 긴 시퀀스를 작은 많은 텍스트 윈도로 변환한다. 이 데이터셋의 각 샘플은 전체 텍스트에서 매우 짧은 부분 문자열이다. RNN은 이 부분문자열 길이만큼만 역전파를 위해 펼쳐진다. 이를 **TBPTT** 라고 한다. window() 메서드를 호출하여 짧은 텍스트 윈도를 갖는 데이터셋을 만들어보자."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "4c3be689",
      "metadata": {
        "id": "4c3be689"
      },
      "outputs": [],
      "source": [
        "# n_steps를 튜닝할 수 있다.\n",
        "# 짧은 입력 시퀀스에서 RNN을 훈련하는 것은 쉽지만 당연히 이 RNN은 n_steps 보다 긴 패턴을 학습할 수 없다.\n",
        "n_steps = 100\n",
        "window_length = n_steps + 1 # target = 한글자앞의 input\n",
        "# window_depth: 윈도우 크기 \n",
        "# shift: 원래는 윈도우크기가 default값이여서 윈도우가 겹치지 않음. shift=1 이면 최대크기의 trainset 생성\n",
        "# drop_remainder: True로 지정하면 같은 크기의 윈도우를 생성함. False면 윈도우크기가 1씩 줄어든다. \n",
        "dataset1 = dataset1.window(window_length, shift=1, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3fdd940",
      "metadata": {
        "id": "b3fdd940"
      },
      "source": [
        "window()메서드를 사용하면 각각 하나의 데이터셋으로 표현되는 윈도를 포함하는 데이터셋을 만든다. 리스트의 리스트와 비슷한 nested dataset이다. 이런 구조는 데이터셋 메서드를 호출하여 각 윈도를 변환할 때 유용하다. 하지만 모델은 데이터셋이 아니라 tensor를 기대하기 때문에 훈련에 데이터셋을 바로 사용할 수 없다. 따라서 flat_map()을 통해서 nested dataset을 flat dataset으로 변환해주어야한다. \n",
        "\n",
        "flat_map()메서드는 중첩 dataset을 평평하게 만들기 전에 각 데이터셋에 적용할 변환 함수를 매개변수로 받을 수 있다. 예를 들어 lambda ds: ds.batch(2) 함수를 flat_map()에 전달하면 텐서 2개를 가진 테이터셋으로 변환해준다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "92c018fc",
      "metadata": {
        "id": "92c018fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "7f538af8-bee5-4ddf-ae03-2c1b096ae1e1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-828d475b3505>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 윈도마다 batch(window_length)를 호출한다. 이 길이는 윈도 길이와 같기 때문에 텐서 하나를 담은 데이터 셋을 얻는다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# 이 데이터셋은 연속된 101글자 길이의 윈도를 담는다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdataset1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[0;34m(self, map_func, name)\u001b[0m\n\u001b[1;32m   2090\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2091\u001b[0m     \"\"\"\n\u001b[0;32m-> 2092\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFlatMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2094\u001b[0m   def interleave(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, name)\u001b[0m\n\u001b[1;32m   5326\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5327\u001b[0m     self._map_func = structured_function.StructuredFunctionWrapper(\n\u001b[0;32m-> 5328\u001b[0;31m         map_func, self._transformation_name(), dataset=input_dataset)\n\u001b[0m\u001b[1;32m   5329\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5330\u001b[0m       raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mfn_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_tf_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefun_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m     \u001b[0;31m# There is no graph to add in eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0madd_to_graph\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2566\u001b[0m     \"\"\"\n\u001b[1;32m   2567\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0;32m-> 2568\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   2569\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2570\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2531\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2532\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2533\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2534\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2535\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2709\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache_key\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_placeholder_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2711\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2712\u001b[0m           self._function_cache.add(cache_key, cache_key_deletion_observer,\n\u001b[1;32m   2713\u001b[0m                                    graph_function)\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m             \u001b[0mautograph_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_autograph_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2636\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m         \u001b[0mspec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1139\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    246\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m    247\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36mwrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    175\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_should_unpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_should_pack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n\n    TypeError: <lambda>() takes 1 positional argument but 2 were given\n"
          ]
        }
      ],
      "source": [
        "# 윈도마다 batch(window_length)를 호출한다. 이 길이는 윈도 길이와 같기 때문에 텐서 하나를 담은 데이터 셋을 얻는다.\n",
        "# 이 데이터셋은 연속된 101글자 길이의 윈도를 담는다.\n",
        "dataset1 = dataset.flat_map(lambda window: window.batch(window_length))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "989f5288",
      "metadata": {
        "id": "989f5288"
      },
      "source": [
        "경사하강법은 훈련 세트 샘플이 동일 독립 분포 일 떄 가장 잘 작동하기 때문에 이 윈도를 섞어야한다. 그다음 윈도를 배치로 만들고 입력과 타깃으로 분리하겠다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "d914b52a",
      "metadata": {
        "id": "d914b52a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "outputId": "68494ed4-a51c-470a-ab84-6bfb61873760"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-d6b4855e64bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdataset1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdataset1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mwindows\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwindows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2046\u001b[0m         warnings.warn(\"The `deterministic` argument has no effect unless the \"\n\u001b[1;32m   2047\u001b[0m                       \"`num_parallel_calls` argument is specified.\")\n\u001b[0;32m-> 2048\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2049\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2050\u001b[0m       return ParallelMapDataset(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m   5245\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5246\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5247\u001b[0;31m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[1;32m   5248\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5249\u001b[0m     variant_tensor = gen_dataset_ops.map_dataset(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mfn_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_tf_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefun_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m     \u001b[0;31m# There is no graph to add in eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0madd_to_graph\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2566\u001b[0m     \"\"\"\n\u001b[1;32m   2567\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0;32m-> 2568\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   2569\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2570\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2531\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2532\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2533\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2534\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2535\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2709\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache_key\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_placeholder_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2711\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2712\u001b[0m           self._function_cache.add(cache_key, cache_key_deletion_observer,\n\u001b[1;32m   2713\u001b[0m                                    graph_function)\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m             \u001b[0mautograph_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_autograph_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2636\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m         \u001b[0mspec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1139\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    246\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m    247\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36mwrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    175\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_should_unpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_should_pack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_fileh4q7wreg.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(windows)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtf__lam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mwindows\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_function_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mlscope\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwindows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lscope'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf__lam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_factory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/core/function_wrappers.py\u001b[0m in \u001b[0;36mwith_function_scope\u001b[0;34m(thunk, scope_name, options)\u001b[0m\n\u001b[1;32m    111\u001b[0m   \u001b[0;34m\"\"\"Inline version of the FunctionScope context manager.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mFunctionScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lambda_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mthunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/__autograph_generated_fileh4q7wreg.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(lscope)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtf__lam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mwindows\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_function_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mlscope\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwindows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lscope'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf__lam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_factory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"<ipython-input-74-d6b4855e64bd>\", line 6, in None  *\n        lambda windows: (windows[:, :-1], windows[:, 1:]))\n\n    TypeError: '_NestedVariant' object is not subscriptable\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "batch_size= 32\n",
        "dataset1 = dataset1.shuffle(10000).batch(batch_size)\n",
        "dataset1 = dataset1.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "cce2cce5",
      "metadata": {
        "id": "cce2cce5"
      },
      "outputs": [],
      "source": [
        "# 범주형 입력특성이므로 원핫인코딩을 진행한다. (고유한 글자 수가 적기 때문에 임베딩 대신 원핫 인코딩을 사용함).\n",
        "dataset1 = dataset1.map(\n",
        "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "414f5c9f",
      "metadata": {
        "id": "414f5c9f"
      },
      "source": [
        "마지막으로 프리페칭을 추가한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "c1455063",
      "metadata": {
        "id": "c1455063"
      },
      "outputs": [],
      "source": [
        "dataset1 = dataset1.prefetch(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "PotQVLqjRPQ0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PotQVLqjRPQ0",
        "outputId": "c87057b4-e7af-41cd-e473-2298ee76f68f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 100, 39) (32, 100)\n"
          ]
        }
      ],
      "source": [
        "for X_batch, Y_batch in dataset.take(1):\n",
        "    print(X_batch.shape, Y_batch.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ee70ba3",
      "metadata": {
        "id": "2ee70ba3"
      },
      "source": [
        "13장에 나온 데이터 전처리와 적재 파트의 공부를 건너 뛰어서 살짝 잘 모르겠는 부분이 있다. 이제 필요성을 느꼈으니 공부를 해야겠다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f9ace11",
      "metadata": {
        "id": "7f9ace11"
      },
      "source": [
        "## Char-RNN 모델 훈련"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cc2a032",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cc2a032",
        "outputId": "1b3a50e7-4765-49c0-cf9d-d0d1726d2cf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "31368/31368 [==============================] - 420s 13ms/step - loss: 1.6238\n",
            "Epoch 2/10\n",
            "31368/31368 [==============================] - 392s 12ms/step - loss: 1.5397\n",
            "Epoch 3/10\n",
            "31368/31368 [==============================] - 396s 13ms/step - loss: 1.5191\n",
            "Epoch 4/10\n",
            "31368/31368 [==============================] - 389s 12ms/step - loss: 1.5076\n",
            "Epoch 5/10\n",
            "31368/31368 [==============================] - 393s 12ms/step - loss: 1.5000\n",
            "Epoch 6/10\n",
            "31368/31368 [==============================] - 411s 13ms/step - loss: 1.4943\n",
            "Epoch 7/10\n",
            "31368/31368 [==============================] - 397s 13ms/step - loss: 1.4906\n",
            "Epoch 8/10\n",
            "31368/31368 [==============================] - 394s 13ms/step - loss: 1.4876\n",
            "Epoch 9/10\n",
            "31368/31368 [==============================] - 390s 12ms/step - loss: 1.4854\n",
            "Epoch 10/10\n",
            "31368/31368 [==============================] - 397s 13ms/step - loss: 1.4828\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# GRU 클래스는 다음 매개변수에서 기본값을 사용할 때에만 GPU를 사용합니다: \n",
        "# activation, recurrent_activation, recurrent_dropout, unroll, use_bias reset_after. 이 때문에 (책과는 달리) recurrent_dropout=0.2를 주석 처리했습니다.\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
        "                   # dropout = 0.2, recurrent_dropout = 0.2),\n",
        "                   dropout = 0.2),\n",
        "    keras.layers.GRU(128, return_sequences=True,\n",
        "                   # dropout = 0.2, recurrent_dropout = 0.2),\n",
        "                   dropout = 0.2),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\n",
        "])\n",
        "\n",
        "\n",
        "# 손실함수는 https://didu-story.tistory.com/27 참조\n",
        "# 라벨이 정수인코딩 된 경우에 사용함\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
        "history = model.fit(dataset, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u_sn43LQ1i7D",
      "metadata": {
        "id": "u_sn43LQ1i7D"
      },
      "outputs": [],
      "source": [
        "model.save(\"my_shakespeare_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "DsI1mwEV13F1",
      "metadata": {
        "id": "DsI1mwEV13F1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "dede7b4a-c5d8-4146-adf8-5df1df9304e8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-e2dea598e7bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"my_shakespeare_model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'No file or directory found at {filepath_str}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: No file or directory found at my_shakespeare_model.h5"
          ]
        }
      ],
      "source": [
        "model = keras.models.load_model(\"my_shakespeare_model.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uK_7366ySR0d",
      "metadata": {
        "id": "uK_7366ySR0d"
      },
      "source": [
        "## Char-RNN 모델 사용하기.\n",
        "이제 셰익스피어가 쓴 텍스트에서 다음 글자를 예측해주는 모델을 얻었다. 이 모델에 새로운 텍스트를 주입하려면 앞에서와 같이 먼저 전처리를 해야한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "hLBhVOVuXYLY",
      "metadata": {
        "id": "hLBhVOVuXYLY"
      },
      "outputs": [],
      "source": [
        "def preprocess(texts):\n",
        "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
        "    return tf.one_hot(X, max_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "JSAt7eoAXkRA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "JSAt7eoAXkRA",
        "outputId": "08b6f1a6-61c4-405e-de0e-d4c4dcfbeb3d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-432d30f79be1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"How are yo\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Y_pred = model.predict_classes(X_new)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequences_to_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_pred\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# 1st sentence, last char\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "X_new = preprocess([\"How are yo\"])\n",
        "#Y_pred = model.predict_classes(X_new)\n",
        "Y_pred = np.argmax(model(X_new), axis=-1)\n",
        "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1] # 1st sentence, last char"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "3LE6kF1J_IGz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LE6kF1J_IGz",
        "outputId": "59f90f10-2144-4637-e904-b1647e5b8590"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
              "        2, 0, 0, 1, 1, 1, 0, 0, 1, 2, 0, 0, 1, 1, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "tf.random.categorical([[np.log(0.5), np.log(0.4), np.log(0.1)]], num_samples=40).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "x5_icqP3h9Ay",
      "metadata": {
        "id": "x5_icqP3h9Ay"
      },
      "source": [
        "## 가짜 셰익스피어 텍스트 생성하기 \n",
        "Char-RNN 모델을 사용해 새로운 텍스트를 생성하려면 먼저 초기 텍스트를 주입하고 모델이 가장 가능성 있는 다음 글자를 예측한다. 이 글자는 텍스트 끝에 추가하고 늘어난 텍스트를 모델에 전달하여 다음 글자를 예측하는 방식이다. 실제로는 이렇게 하면 같은 단어가 계속 반복될 수 있다. 대신 tf.random.categorical()함수를 이용해 모델이 추정한 확률을 기반으로 다음 글자를 무작위로 선택할 수 있다.\n",
        "\n",
        "categorical() 함수는 클래스의 로그 확률(로직)을 전달하면 랜덤하게 클래스 인덱스를 샘플링한다. 생성된 텍스트의 다양성을 더 많이제어하려면 temperature 라고 불리는 숫자로 로짓을 나눈다. 온도가 낮을수록 랜덤성이 줄고 온도가 높을 수록 랜덤성이 높아진다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "Y3IQLQoXis5h",
      "metadata": {
        "id": "Y3IQLQoXis5h"
      },
      "outputs": [],
      "source": [
        "# 위 방식을 사용해서 다음 글자를 선택하고 그 텍스트를 반환한다.\n",
        "def next_char(text, temperature=1):\n",
        "\n",
        "    # X_new 형태와 y_proba 형태 살펴보기\n",
        "    \n",
        "    \n",
        "    X_new = preprocess([text])\n",
        "    y_proba = model(X_new)[0, -1:, :]\n",
        "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
        "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
        "    return tokenizer.sequences_to_texts(char_id.numpy())[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "pJL2cjmplPky",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "pJL2cjmplPky",
        "outputId": "a25d1f01-7929-44be-de10-5f60fc592778"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-37a826d4d011>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnext_char\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"How are yo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-772ff11feff2>\u001b[0m in \u001b[0;36mnext_char\u001b[0;34m(text, temperature)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mX_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0my_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mrescaled_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_proba\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mchar_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrescaled_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "next_char(\"How are yo\", temperature=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "dZSiqbQYjOwR",
      "metadata": {
        "id": "dZSiqbQYjOwR"
      },
      "outputs": [],
      "source": [
        "# 위 함수를 반복 호출하고 텍스트뒤에 합쳐주는 함수\n",
        "def complete_text(text, n_chars=50, temperature=1):\n",
        "    for _ in range(n_chars):\n",
        "        text += next_char(text, temperature)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "ettMzQqvlNXA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "ettMzQqvlNXA",
        "outputId": "3dc3ddf6-4cf1-4ce4-f77d-8228c2e25b92"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-36ab1904392d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomplete_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-26-b305d9a8ef56>\u001b[0m in \u001b[0;36mcomplete_text\u001b[0;34m(text, n_chars, temperature)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcomplete_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_chars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_chars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnext_char\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-772ff11feff2>\u001b[0m in \u001b[0;36mnext_char\u001b[0;34m(text, temperature)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mX_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0my_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mrescaled_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_proba\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mchar_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrescaled_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "print(complete_text(\"t\", temperature=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IroxkaIXlVtf",
      "metadata": {
        "id": "IroxkaIXlVtf"
      },
      "source": [
        "## 상태가 있는 RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca7f0e86",
      "metadata": {
        "id": "ca7f0e86"
      },
      "source": [
        "지금까지는 상태가 없는 RNN을 사용했다. 훈련 반복마다 모델의 은닉 상태를 0으로 초기화한다. 타임 스텝마다 이 상태를 업데이트하고 마지막 타임 스텝후에는 더 필요가 없기 때문에 버린다. RNN이 한 훈련 배치를 처리한 후에 마지막 상태를 다음 훈련 배치의 초기 상태로 사용하면 어떨까? 이렇게 하면 역전파는 짧은 시퀀스 안에서만 일어나지만 모델은 장기간 패턴을 학습 가능하다. 이를 **상태가 있는 RNN**이라고 한다.\n",
        "\n",
        "먼저 상태가 있는 RNN은 배치에 있는 각 입력 시퀀스가 이전 배치의 시퀀스가 끝난 지점에서 시작해야한다. 따라서 상태가 있는 RNN을 만들기 위해 첫 번쨰로 할 일은 순차적이고 겹치지 않는 시퀀스를 만드는 것이다. (상태가 없는 RNN에서 사용된 겹치는 부분이 있는 Dataset이면 안된다). Dataset을 만들 때 window() 메서드에서 shift=n_steps로 지정해주면 된다. 또한 shuffle()메서드를 호출해서는 안된다. 안타깝게도 상태가 있는 RNN을 위한 데이터 셋은 상태가 없는 RNN의 경우보다 배치를 구성하기 더 힘들다. 실제로 batch(32)라고 호출하면 32개의 연속적인 윈도가 같은 배치에 들어간다. 이 윈도가 끝난 지점부터 다음배치가 계속되지 않는다. 첫 번째 배치는 1~32까지 포함하고 두번째 배치는 윈도 33부터 64까지 포함한다. 따라서 각 배치의 첫 번째 윈도를 생각하면 연속적이지 않음을 할 수 있다. 이 문제의 가장 간단한 해결책은 하나의 윈도를 갖는 배치를 만드는 것이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "0f524a68",
      "metadata": {
        "id": "0f524a68"
      },
      "outputs": [],
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
        "dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
        "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
        "dataset = dataset.batch(1)\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
        "dataset = dataset.map(\n",
        "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
        "dataset = dataset.prefetch(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "326cc6c2",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "326cc6c2",
        "outputId": "dc5690f4-ad04-40b2-d893-0c90e19813e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 100, 39) (1, 100)\n",
            "tf.Tensor(\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]], shape=(100, 39), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "for X_batch, Y_batch in dataset.take(1):\n",
        "    print(X_batch.shape, Y_batch.shape)\n",
        "    print((X_batch[:][:][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "b0738df7",
      "metadata": {
        "id": "b0738df7"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "encoded_parts = np.array_split(encoded[:train_size], batch_size)\n",
        "datasets = []\n",
        "for encoded_part in encoded_parts:\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(encoded_part)\n",
        "    dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
        "    dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
        "    datasets.append(dataset)\n",
        "dataset = tf.data.Dataset.zip(tuple(datasets)).map(lambda *windows: tf.stack(windows))\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
        "dataset = dataset.map(\n",
        "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
        "dataset = dataset.prefetch(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "f1b12be9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1b12be9",
        "outputId": "39df8d1f-45d1-4ca3-c528-503c14f2b195"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 100, 39) (32, 100)\n",
            "tf.Tensor(\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]], shape=(100, 39), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "for X_batch, Y_batch in dataset.take(1):\n",
        "    print(X_batch.shape, Y_batch.shape)\n",
        "    print((X_batch[:][:][1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "6d49fa0d",
      "metadata": {
        "id": "6d49fa0d"
      },
      "outputs": [],
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
        "                    dropout=0.2, batch_input_shape=[batch_size, None, max_id]),\n",
        "                    # recurrent_dropout=0.2)\n",
        "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
        "                    dropout=0.2),\n",
        "                     # recurrent_dropout=0.2)\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
        "                                                   activation=\"softmax\"))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "31bbb2dc",
      "metadata": {
        "id": "31bbb2dc"
      },
      "outputs": [],
      "source": [
        "# 아무리 stateful RNN이라고 해도 에포크가 시작할 때는 state를 초기화해주어야한다.\n",
        "# stateless RNN은 배치마다 state를 초기화하고, stateful RNN은 에포크마다 state를 초기화한다.\n",
        "\n",
        "class ResetStatesCallback(keras.callbacks.Callback):\n",
        "    def on_epoch_begin(self, epoch, logs):\n",
        "        self.model.reset_states()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
        "history = model.fit(dataset, epochs=50,\n",
        "                    callbacks=[ResetStatesCallback()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_sgx3PcrZsL",
        "outputId": "6055ef23-21f1-4e23-8987-ee3f8f8a339d"
      },
      "id": "-_sgx3PcrZsL",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "313/313 [==============================] - 12s 15ms/step - loss: 2.6211\n",
            "Epoch 2/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 2.2422\n",
            "Epoch 3/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 2.1109\n",
            "Epoch 4/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 2.0358\n",
            "Epoch 5/50\n",
            "313/313 [==============================] - 6s 21ms/step - loss: 1.9854\n",
            "Epoch 6/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.9480\n",
            "Epoch 7/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.9206\n",
            "Epoch 8/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.8991\n",
            "Epoch 9/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.8817\n",
            "Epoch 10/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.8661\n",
            "Epoch 11/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.8533\n",
            "Epoch 12/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.8419\n",
            "Epoch 13/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.8338\n",
            "Epoch 14/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.8235\n",
            "Epoch 15/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.8160\n",
            "Epoch 16/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.8100\n",
            "Epoch 17/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.8012\n",
            "Epoch 18/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.7960\n",
            "Epoch 19/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.7923\n",
            "Epoch 20/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.7885\n",
            "Epoch 21/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.7840\n",
            "Epoch 22/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.7787\n",
            "Epoch 23/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.7739\n",
            "Epoch 24/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.7714\n",
            "Epoch 25/50\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 1.7683\n",
            "Epoch 26/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.7639\n",
            "Epoch 27/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.7626\n",
            "Epoch 28/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.7589\n",
            "Epoch 29/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.7567\n",
            "Epoch 30/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.7535\n",
            "Epoch 31/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.7490\n",
            "Epoch 32/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.7480\n",
            "Epoch 33/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.7451\n",
            "Epoch 34/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.7456\n",
            "Epoch 35/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.7407\n",
            "Epoch 36/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.7402\n",
            "Epoch 37/50\n",
            "313/313 [==============================] - 5s 18ms/step - loss: 1.7354\n",
            "Epoch 38/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.7376\n",
            "Epoch 39/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.7342\n",
            "Epoch 40/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.7310\n",
            "Epoch 41/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.7307\n",
            "Epoch 42/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.7302\n",
            "Epoch 43/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.7282\n",
            "Epoch 44/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.7274\n",
            "Epoch 45/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.7254\n",
            "Epoch 46/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.7209\n",
            "Epoch 47/50\n",
            "313/313 [==============================] - 5s 14ms/step - loss: 1.7232\n",
            "Epoch 48/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.7200\n",
            "Epoch 49/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.7222\n",
            "Epoch 50/50\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.7188\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"my_shakespeare_stateful_model.h5\")"
      ],
      "metadata": {
        "id": "WydP4LDTEK7S"
      },
      "id": "WydP4LDTEK7S",
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 감성분석\n",
        "이전까지는 글자 수준 모델을 만들었다. 이제 단어 수준 모델을 살펴보고 자주 등장하는 자연어 처리 작업인 감성분석을 다루어 보자. 이 과정에서 마스킹을 사용해 길이가 다른 시퀀스를 다루는 방법도 알아보자.\n",
        "\n",
        "학습은 IMDb 리뷰 데이터 셋을 사용한다."
      ],
      "metadata": {
        "id": "nE1opKYpFv8n"
      },
      "id": "nE1opKYpFv8n"
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data()\n",
        "X_train[0][:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuvJCD2QVxAd",
        "outputId": "9aa5c962-2237-4e05-ae0e-87c615ead909"
      },
      "id": "iuvJCD2QVxAd",
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "리뷰 내용은 어디 있을까? 사실 여기서 볼 수 있듯 이 데이터 셋은 이미 전처리되어있다. X_train은 리뷰들의 리스트이다. 각 리뷰는 넘파이 정수 배열로 표현된다. 각 정수는 하나의 단어를 나타낸다. 구두점을 모두 제거하고 단어는 소문자로 변환한 다음 공백으로 나누어 빈도에 따라 인덱스를 붙힌다. (따라서 정수가 낮을 수록 자주등장하는 단어이다). 정수 0, 1, 2는 특별하다. 각각 패딩토큰, SOS(start of sequence)토큰, 알 수 없는 단어를 의미한다. 리뷰 내용을 보고 싶다면 다음과 같이 디코딩할 수 있다."
      ],
      "metadata": {
        "id": "BGxdsrhqV-Iz"
      },
      "id": "BGxdsrhqV-Iz"
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = keras.datasets.imdb.get_word_index()\n",
        "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
        "for id_, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
        "    id_to_word[id_] = token\n",
        "\n",
        "\" \".join([id_to_word[id_] for id_ in X_train[0][:10]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "S4_MuW5TW0pO",
        "outputId": "a93dcb07-e296-4438-cd35-fe65d6e7ab06"
      },
      "id": "S4_MuW5TW0pO",
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1641221/1641221 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<sos> this film was just brilliant casting location scenery story'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "시"
      ],
      "metadata": {
        "id": "6eovUwJMXVY1"
      },
      "id": "6eovUwJMXVY1"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}