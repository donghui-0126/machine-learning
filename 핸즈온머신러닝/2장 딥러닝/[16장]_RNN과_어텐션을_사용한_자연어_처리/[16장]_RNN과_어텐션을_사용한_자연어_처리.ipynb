{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donghui-0126/machine-learning/blob/main/%ED%95%B8%EC%A6%88%EC%98%A8%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/2%EC%9E%A5%20%EB%94%A5%EB%9F%AC%EB%8B%9D/%5B16%EC%9E%A5%5D_RNN%EA%B3%BC_%EC%96%B4%ED%85%90%EC%85%98%EC%9D%84_%EC%82%AC%EC%9A%A9%ED%95%9C_%EC%9E%90%EC%97%B0%EC%96%B4_%EC%B2%98%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f3843735",
      "metadata": {
        "id": "f3843735"
      },
      "outputs": [],
      "source": [
        "# -*- conding:utf-8 -*-\n",
        "\n",
        "# 파이썬 ≥3.5 필수\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# 사이킷런 ≥0.20 필수\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "# 텐서플로 ≥2.0 필수\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"감지된 GPU가 없습니다. GPU가 없으면 CNN은 매우 느릴 수 있습니다.\")\n",
        "    \n",
        "\n",
        "# 공통 모듈 임포트\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# 노트북 실행 결과를 동일하게 유지하기 위해\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 깔끔한 그래프 출력을 위해\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "427271a7",
      "metadata": {
        "id": "427271a7"
      },
      "source": [
        "이번 장에서는 자연어를 읽고 쓰는 AI를 만드는 법을 알아볼 것이다. \n",
        "\n",
        "자연어 문제를 위해 많이 사용하는 방법은 순환 신경망이다. 따라서 RNN을 계속해서 살펴본다. 문장에서 다음글자를 예측하도록 훈련하는 **문자 단위 RNN(character RNN)** 부터 시작한다. 새로운 텍스트를 생성하고 그 과정에서 매우 긴 시퀀스를 가진 텐서플로 데이터셋을 만드는 방법을 알아보자. 먼저 **상태가 없는 RNN(stateless RNN)** 을 사용하고 다음에 **상태가 있는 RNN(stateful RNN)** 을 구축할 것이다. (상태가 없는 RNN은 각 반복에서 무작위하게 택한 텍스트의 일부분으로 학습하고, 나머지 텍스트에서 어떤 정보도 사용하지 않는다. 상태가 있는 RNN은 훈련 반복 사이에 은닉 상태를 유지하고 중지된 곳에서 이어서 상태를 반영한다. 그래서 더 긴패턴을 학습할 수 있다). 그 다음에는 감성분석을 수행하는 RNN을 구축하고 이번에는 문자가 아니라 단어의 시퀀스로 문장을 다룰것이다. 그리고 신경망 기계번역(NMT)을 수행할 수 있는 인코더-디코더 구조를 만들기 위해 RNN을 사용하는 방법을 알아보자. 도구로는 텐서플로 애드온 프로젝트에서 제공하는 seq2seq API를 사용한다.\n",
        "\n",
        "16.4절에서는 어텐션 매커니즘을 알아보자. 이름에서 알 수 있듯이 이는 각 타임 스텝에서 모델이 집중해야 할 입력 부분을 선택하도록 학습하는 신경망 구성요소이다. 먼저 어텐션을 사용하여 RNN 기반의 인코더-디코더 구조의 성능을 높이는 방법을 알아본 뒤, RNN을 모두 제거하고 어텐션만 사용해 매우좋은 성능을 내는 **트랜스포머** 라는 구조도 살펴보자.  마지막으로 NLP분야에서 가장 중요한 발전을 살펴보자. 트랜스포머를 시반으로 한 GPT-2와 BERT같은 매우 강력한 언어 모델도 포함한다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "348a1c7b",
      "metadata": {
        "id": "348a1c7b"
      },
      "source": [
        "# char-RNN을 사용해 셰익스피어 문체 생성하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "996d1d9f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "996d1d9f",
        "outputId": "1b4e2956-ee68-4b19-d245-55e7e0904a8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://homl.info/shakespeare\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "shakespeare_url = \"https://homl.info/shakespeare\"  # 단죽 url\n",
        "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
        "with open(filepath) as f:\n",
        "    shakespeare_text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1ad9e061",
      "metadata": {
        "id": "1ad9e061"
      },
      "outputs": [],
      "source": [
        "# 모든 글자를 정수로 인코딩해준다. \n",
        "# 간단하게 Tokenizer 클래스를 사용한다.\n",
        "\n",
        "# char_level 을 True로 지정해서 단어 수준 인코딩 대신 글자 수준 인코딩을 만든다. 이 클래스는 기본적으로 텍스트를 소문자로 바꾼다.\n",
        "# 이제 문장을 글자 ID로 인코딩 하거나 반대로 디코딩 할 수 있다. \n",
        "# 이를 통해텍스트에 있는 고유 글자 개수와 전체 글자 개수를 알 수 있다.\n",
        "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(shakespeare_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "72c34059",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72c34059",
        "outputId": "111a1d14-3402-4acc-ed01-9b7fb7c937db"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[20, 6, 9, 8, 3]]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "tokenizer.texts_to_sequences([\"First\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e24d084b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e24d084b",
        "outputId": "e68a1599-d325-48c4-a382-d5901aeb9273"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['f i r s t']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "tokenizer.sequences_to_texts([[20,6,9,8,3]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e7695e76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7695e76",
        "outputId": "ef4dd77d-4307-4d9b-b30d-2a68a64451e3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# 고유 글자 개수\n",
        "max_id = len(tokenizer.word_index)\n",
        "max_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ba9b5ef9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba9b5ef9",
        "outputId": "822079b3-2fd9-453e-a42e-3dc2a97413a5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1115394"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# 전체 글자 개수\n",
        "dataset_size = tokenizer.document_count \n",
        "dataset_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a6d7f28c",
      "metadata": {
        "id": "a6d7f28c"
      },
      "outputs": [],
      "source": [
        "# 전체 텍스트를 인코딩해서 각 글자를 ID로 나타내 보자.\n",
        "# 1~ 39인 값을 0~38로 바꾸기\n",
        "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "49bf0e39",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49bf0e39",
        "outputId": "7a7c3be9-89df-45fc-b27d-6a83e5776d88"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1115394,)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "encoded.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7f9a744",
      "metadata": {
        "id": "e7f9a744"
      },
      "source": [
        "## 순차 데이터셋을 나누는 방법\n",
        "훈련셋, 테스트셋, 검증셋을 중복되지 않도록 만드는 것은 중요하다.\n",
        "\n",
        "보통 시계열을 다룰 때는 시간에 따라 나눈다. 다른 차원으로 나눌 수 있지만, train set 과 test set 사이의 상관관계가 있다면 오차를 낙관적으로 측정해서 모델이 유용하지 않을 것이다. \n",
        "\n",
        "시계열 데이터가 시간에 따라서 반복되는 **변하지 않는 상태** 라면 시계열을 안정적으로 분석할 수 있다. 하지만 그렇지 않은 시계열도 있을 것이다. \n",
        "\n",
        "시계열이 안정적인지 확인할 방법은 시간에 따라 검증 세트에 대한 모델의 오차를 그리는 것이다. 모델이 검증 세트 마지막 보다 첫 부분에서 성능이 더 좋다면 이 시계열이 충분히 안정되지 않을 것일 수 있다. 이런 경우에는 **더 짧은 사건 간격으로** 모델을 훈련하는 게 좋다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a4160b18",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4160b18",
        "outputId": "a074d226-3e5a-47cf-ab55-f4a594101fce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1003854"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# 셰익스피어 텍스트의 처음 90%를 trainset으로 사용하고 나머지는 validset과 testset로 사용한다. \n",
        "# 이 set에서 한 번에 한 글자씩 반환하는 tf.data.Dataset 객체를 만든다.\n",
        "train_size = dataset_size * 90 // 100\n",
        "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
        "train_size"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f04f7ee",
      "metadata": {
        "id": "6f04f7ee"
      },
      "source": [
        "## 순차 데이터를 윈도 여러개로 자르기\n",
        "\n",
        "https://teddylee777.github.io/tensorflow/dataset-batch-window  이해가 잘 안되서 찾아본 블로그이다. \n",
        "\n",
        "trainset은 백만개 이상의 글자로 이루어진 시퀀스 하나이다. 여기에 신경망을 직접 훈련시킬수는 없다. 이 RNN은 백만 개의 층이 있는 심층 신경망과 비슷하고 (매우 긴) 샘플 하나로 훈련하는 셈이 된다. 대신 데이터셋의 window() 메서드를 사용해 이 긴 시퀀스를 작은 많은 텍스트 윈도로 변환한다. 이 데이터셋의 각 샘플은 전체 텍스트에서 매우 짧은 부분 문자열이다. RNN은 이 부분문자열 길이만큼만 역전파를 위해 펼쳐진다. 이를 **TBPTT** 라고 한다. window() 메서드를 호출하여 짧은 텍스트 윈도를 갖는 데이터셋을 만들어보자."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4c3be689",
      "metadata": {
        "id": "4c3be689"
      },
      "outputs": [],
      "source": [
        "# n_steps를 튜닝할 수 있다.\n",
        "# 짧은 입력 시퀀스에서 RNN을 훈련하는 것은 쉽지만 당연히 이 RNN은 n_steps 보다 긴 패턴을 학습할 수 없다.\n",
        "n_steps = 100\n",
        "window_length = n_steps + 1 # target = 한글자앞의 input\n",
        "# window_depth: 윈도우 크기 \n",
        "# shift: 원래는 윈도우크기가 default값이여서 윈도우가 겹치지 않음. shift=1 이면 최대크기의 trainset 생성\n",
        "# drop_remainder: True로 지정하면 같은 크기의 윈도우를 생성함. False면 윈도우크기가 1씩 줄어든다. \n",
        "dataset = dataset.window(window_length, shift=1, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3fdd940",
      "metadata": {
        "id": "b3fdd940"
      },
      "source": [
        "window()메서드를 사용하면 각각 하나의 데이터셋으로 표현되는 윈도를 포함하는 데이터셋을 만든다. 리스트의 리스트와 비슷한 nested dataset이다. 이런 구조는 데이터셋 메서드를 호출하여 각 윈도를 변환할 때 유용하다. 하지만 모델은 데이터셋이 아니라 tensor를 기대하기 때문에 훈련에 데이터셋을 바로 사용할 수 없다. 따라서 flat_map()을 통해서 nested dataset을 flat dataset으로 변환해주어야한다. \n",
        "\n",
        "flat_map()메서드는 중첩 dataset을 평평하게 만들기 전에 각 데이터셋에 적용할 변환 함수를 매개변수로 받을 수 있다. 예를 들어 lambda ds: ds.batch(2) 함수를 flat_map()에 전달하면 텐서 2개를 가진 테이터셋으로 변환해준다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "92c018fc",
      "metadata": {
        "id": "92c018fc"
      },
      "outputs": [],
      "source": [
        "# 윈도마다 batch(window_length)를 호출한다. 이 길이는 윈도 길이와 같기 때문에 텐서 하나를 담은 데이터 셋을 얻는다.\n",
        "# 이 데이터셋은 연속된 101글자 길이의 윈도를 담는다.\n",
        "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "989f5288",
      "metadata": {
        "id": "989f5288"
      },
      "source": [
        "경사하강법은 훈련 세트 샘플이 동일 독립 분포 일 떄 가장 잘 작동하기 때문에 이 윈도를 섞어야한다. 그다음 윈도를 배치로 만들고 입력과 타깃으로 분리하겠다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d914b52a",
      "metadata": {
        "id": "d914b52a"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "batch_size= 32\n",
        "dataset = dataset.shuffle(10000).batch(batch_size)\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "cce2cce5",
      "metadata": {
        "id": "cce2cce5"
      },
      "outputs": [],
      "source": [
        "# 범주형 입력특성이므로 원핫인코딩을 진행한다. (고유한 글자 수가 적기 때문에 임베딩 대신 원핫 인코딩을 사용함).\n",
        "dataset = dataset.map(\n",
        "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "414f5c9f",
      "metadata": {
        "id": "414f5c9f"
      },
      "source": [
        "마지막으로 프리페칭을 추가한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c1455063",
      "metadata": {
        "id": "c1455063"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.prefetch(1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for X_batch, Y_batch in dataset.take(1):\n",
        "    print(X_batch.shape, Y_batch.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PotQVLqjRPQ0",
        "outputId": "05065d8f-a60c-4339-c2e4-344a1083dd33"
      },
      "id": "PotQVLqjRPQ0",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 100, 39) (32, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ee70ba3",
      "metadata": {
        "id": "2ee70ba3"
      },
      "source": [
        "13장에 나온 데이터 전처리와 적재 파트의 공부를 건너 뛰어서 살짝 잘 모르겠는 부분이 있다. 이제 필요성을 느꼈으니 공부를 해야겠다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f9ace11",
      "metadata": {
        "id": "7f9ace11"
      },
      "source": [
        "## Char-RNN 모델 훈련"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "1cc2a032",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cc2a032",
        "outputId": "1b3a50e7-4765-49c0-cf9d-d0d1726d2cf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "31368/31368 [==============================] - 420s 13ms/step - loss: 1.6238\n",
            "Epoch 2/10\n",
            "31368/31368 [==============================] - 392s 12ms/step - loss: 1.5397\n",
            "Epoch 3/10\n",
            "31368/31368 [==============================] - 396s 13ms/step - loss: 1.5191\n",
            "Epoch 4/10\n",
            "31368/31368 [==============================] - 389s 12ms/step - loss: 1.5076\n",
            "Epoch 5/10\n",
            "31368/31368 [==============================] - 393s 12ms/step - loss: 1.5000\n",
            "Epoch 6/10\n",
            "31368/31368 [==============================] - 411s 13ms/step - loss: 1.4943\n",
            "Epoch 7/10\n",
            "31368/31368 [==============================] - 397s 13ms/step - loss: 1.4906\n",
            "Epoch 8/10\n",
            "31368/31368 [==============================] - 394s 13ms/step - loss: 1.4876\n",
            "Epoch 9/10\n",
            "31368/31368 [==============================] - 390s 12ms/step - loss: 1.4854\n",
            "Epoch 10/10\n",
            "31368/31368 [==============================] - 397s 13ms/step - loss: 1.4828\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# GRU 클래스는 다음 매개변수에서 기본값을 사용할 때에만 GPU를 사용합니다: \n",
        "# activation, recurrent_activation, recurrent_dropout, unroll, use_bias reset_after. 이 때문에 (책과는 달리) recurrent_dropout=0.2를 주석 처리했습니다.\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
        "                   # dropout = 0.2, recurrent_dropout = 0.2),\n",
        "                   dropout = 0.2),\n",
        "    keras.layers.GRU(128, return_sequences=True,\n",
        "                   # dropout = 0.2, recurrent_dropout = 0.2),\n",
        "                   dropout = 0.2),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\n",
        "])\n",
        "\n",
        "\n",
        "# 손실함수는 https://didu-story.tistory.com/27 참조\n",
        "# 라벨이 정수인코딩 된 경우에 사용함\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
        "history = model.fit(dataset, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"my_shakespeare_model.h5\")"
      ],
      "metadata": {
        "id": "u_sn43LQ1i7D"
      },
      "id": "u_sn43LQ1i7D",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model(\"my_shakespeare_model.h5\")"
      ],
      "metadata": {
        "id": "DsI1mwEV13F1"
      },
      "id": "DsI1mwEV13F1",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Char-RNN 모델 사용하기.\n",
        "이제 셰익스피어가 쓴 텍스트에서 다음 글자를 예측해주는 모델을 얻었다. 이 모델에 새로운 텍스트를 주입하려면 앞에서와 같이 먼저 전처리를 해야한다."
      ],
      "metadata": {
        "id": "uK_7366ySR0d"
      },
      "id": "uK_7366ySR0d"
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(texts):\n",
        "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
        "    return tf.one_hot(X, max_id)"
      ],
      "metadata": {
        "id": "hLBhVOVuXYLY"
      },
      "id": "hLBhVOVuXYLY",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_new = preprocess([\"How are yo\"])\n",
        "#Y_pred = model.predict_classes(X_new)\n",
        "Y_pred = np.argmax(model(X_new), axis=-1)\n",
        "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1] # 1st sentence, last char"
      ],
      "metadata": {
        "id": "JSAt7eoAXkRA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2087d7cf-889c-47e2-d03b-8c7edb754a1e"
      },
      "id": "JSAt7eoAXkRA",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'u'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "tf.random.categorical([[np.log(0.5), np.log(0.4), np.log(0.1)]], num_samples=40).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LE6kF1J_IGz",
        "outputId": "e2779f3c-5a7c-44ce-c0c1-0f8eb4388078"
      },
      "id": "3LE6kF1J_IGz",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
              "        2, 0, 0, 1, 1, 1, 0, 0, 1, 2, 0, 0, 1, 1, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 가짜 셰익스피어 텍스트 생성하기 \n",
        "Char-RNN 모델을 사용해 새로운 텍스트를 생성하려면 먼저 초기 텍스트를 주입하고 모델이 가장 가능성 있는 다음 글자를 예측한다. 이 글자는 텍스트 끝에 추가하고 늘어난 텍스트를 모델에 전달하여 다음 글자를 예측하는 방식이다. 실제로는 이렇게 하면 같은 단어가 계속 반복될 수 있다. 대신 tf.random.categorical()함수를 이용해 모델이 추정한 확률을 기반으로 다음 글자를 무작위로 선택할 수 있다.\n",
        "\n",
        "categorical() 함수는 클래스의 로그 확률(로직)을 전달하면 랜덤하게 클래스 인덱스를 샘플링한다. 생성된 텍스트의 다양성을 더 많이제어하려면 temperature 라고 불리는 숫자로 로짓을 나눈다. 온도가 낮을수록 랜덤성이 줄고 온도가 높을 수록 랜덤성이 높아진다. "
      ],
      "metadata": {
        "id": "x5_icqP3h9Ay"
      },
      "id": "x5_icqP3h9Ay"
    },
    {
      "cell_type": "code",
      "source": [
        "# 위 방식을 사용해서 다음 글자를 선택하고 그 텍스트를 반환한다.\n",
        "def next_char(text, temperature=1):\n",
        "\n",
        "    # X_new 형태와 y_proba 형태 살펴보기\n",
        "    \n",
        "    \n",
        "    X_new = preprocess([text])\n",
        "    y_proba = model(X_new)[0, -1:, :]\n",
        "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
        "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
        "    return tokenizer.sequences_to_texts(char_id.numpy())[0]"
      ],
      "metadata": {
        "id": "Y3IQLQoXis5h"
      },
      "id": "Y3IQLQoXis5h",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "next_char(\"How are yo\", temperature=1)"
      ],
      "metadata": {
        "id": "pJL2cjmplPky",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "02605ef0-fa0f-4894-f398-371933eaefe5"
      },
      "id": "pJL2cjmplPky",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'u'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 위 함수를 반복 호출하고 텍스트뒤에 합쳐주는 함수\n",
        "def complete_text(text, n_chars=50, temperature=1):\n",
        "    for _ in range(n_chars):\n",
        "        text += next_char(text, temperature)\n",
        "    return text"
      ],
      "metadata": {
        "id": "dZSiqbQYjOwR"
      },
      "id": "dZSiqbQYjOwR",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(complete_text(\"t\", temperature=1))"
      ],
      "metadata": {
        "id": "ettMzQqvlNXA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0cd02eb-676c-44ef-920a-bf4eef4012be"
      },
      "id": "ettMzQqvlNXA",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the off,\n",
            "did he er this verona on a kind of yoursel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 상태가 있는 RNN"
      ],
      "metadata": {
        "id": "IroxkaIXlVtf"
      },
      "id": "IroxkaIXlVtf"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jreTMyPvt7zO"
      },
      "id": "jreTMyPvt7zO",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}